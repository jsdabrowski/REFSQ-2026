{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6399a24f-77aa-42f3-97f2-acc9ebab61a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 512 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Zero_Shot_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2 (Zero_Shot_Prompt): 100%|██████████████████████████| 512/512 [1:06:02<00:00,  7.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama2 using Zero_Shot_Prompt completed in 66.04 minutes\n",
      "\n",
      "--- Sample of Predictions for llama2 (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0  feature request  \n",
      "1  feature request  \n",
      "2  feature request  \n",
      "3  feature request  \n",
      "4  feature request  \n",
      "\n",
      "--- Classification Report for llama2 (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.10      0.98      0.18        50\n",
      "     bug report       0.00      0.00      0.00       288\n",
      "          other       0.75      0.10      0.18       174\n",
      "\n",
      "       accuracy                           0.13       512\n",
      "      macro avg       0.28      0.36      0.12       512\n",
      "   weighted avg       0.26      0.13      0.08       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama2 (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral (Zero_Shot_Prompt): 100%|█████████████████████████| 512/512 [2:02:22<00:00, 14.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with mistral using Zero_Shot_Prompt completed in 122.38 minutes\n",
      "\n",
      "--- Sample of Predictions for mistral (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report       other\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.27      0.70      0.38        50\n",
      "     bug report       0.92      0.49      0.63       288\n",
      "          other       0.61      0.79      0.69       174\n",
      "\n",
      "       accuracy                           0.61       512\n",
      "      macro avg       0.60      0.66      0.57       512\n",
      "   weighted avg       0.75      0.61      0.63       512\n",
      "\n",
      "\n",
      "========== Evaluation for mistral (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b (Zero_Shot_Prompt): 100%|███████████████████████| 512/512 [2:13:22<00:00, 15.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama3:8b using Zero_Shot_Prompt completed in 133.38 minutes\n",
      "\n",
      "--- Sample of Predictions for llama3:8b (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report       other\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.78      0.36      0.49        50\n",
      "     bug report       0.79      0.91      0.84       288\n",
      "          other       0.77      0.70      0.73       174\n",
      "\n",
      "       accuracy                           0.78       512\n",
      "      macro avg       0.78      0.66      0.69       512\n",
      "   weighted avg       0.78      0.78      0.77       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama3:8b (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b (Zero_Shot_Prompt): 100%|████████████████████████| 512/512 [3:40:16<00:00, 25.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with gemma:7b using Zero_Shot_Prompt completed in 220.28 minutes\n",
      "\n",
      "--- Sample of Predictions for gemma:7b (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for gemma:7b (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.33      0.70      0.45        50\n",
      "     bug report       0.86      0.79      0.82       288\n",
      "          other       0.78      0.64      0.70       174\n",
      "\n",
      "       accuracy                           0.73       512\n",
      "      macro avg       0.66      0.71      0.66       512\n",
      "   weighted avg       0.78      0.73      0.75       512\n",
      "\n",
      "\n",
      "========== Evaluation for gemma:7b (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: phi3:mini ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with phi3:mini (Zero_Shot_Prompt): 100%|███████████████████████| 512/512 [2:38:07<00:00, 18.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with phi3:mini using Zero_Shot_Prompt completed in 158.13 minutes\n",
      "\n",
      "--- Sample of Predictions for phi3:mini (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for phi3:mini (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.57      0.08      0.14        50\n",
      "     bug report       0.69      0.95      0.80       288\n",
      "          other       0.80      0.51      0.62       174\n",
      "\n",
      "       accuracy                           0.71       512\n",
      "      macro avg       0.69      0.51      0.52       512\n",
      "   weighted avg       0.72      0.71      0.68       512\n",
      "\n",
      "\n",
      "========== Evaluation for phi3:mini (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "\n",
      "========== ALL EVALUATION COMPLETE ==========\n",
      "\n",
      "Summary of Accuracies across all Prompt Strategies and Models:\n",
      "\n",
      "--- Prompt Strategy: Zero_Shot_Prompt ---\n",
      "  llama2: Accuracy = 0.13\n",
      "  mistral: Accuracy = 0.61\n",
      "  llama3:8b: Accuracy = 0.78\n",
      "  gemma:7b: Accuracy = 0.73\n",
      "  phi3:mini: Accuracy = 0.71\n",
      "\n",
      "--- Final Evaluation End ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Zero_Shot_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis for user feedback. Classify the given app review into exactly ONE of:\n",
    "- Feature Request\n",
    "- Bug Report\n",
    "- Other\n",
    "\n",
    "DEFINITIONS (concise):\n",
    "- Feature Request: Asks for a new capability or an enhancement to something that currently works.\n",
    "- Bug Report: Describes an error, crash, failure, or behavior not working as intended.\n",
    "- Other: Vague praise/complaint, questions, off-topic, or not enough info to decide.\n",
    "\n",
    "TIE-BREAK RULES:\n",
    "- If any error/failure is stated or implied → Bug Report.\n",
    "- If any explicit request/suggestion for new/improved functionality and no error → Feature Request.\n",
    "- Otherwise → Other.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return ONLY one of these exact strings with no extra text or punctuation:\n",
    "Feature Request | Bug Report | Other\n",
    "\n",
    "App Review Segment: '''{review_text}'''\n",
    "Answer:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11195b3d-9cd5-45b3-9fed-1f1ed76f7232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 512 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Zero_Shot_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2 (Zero_Shot_Prompt):  90%|███████████████████████▎  | 460/512 [1:32:43<12:28, 14.39s/it]2025-09-28 14:44:24,662 - __main__ - ERROR - Ollama request timed out for review: ''With YouTube playing while screens locked'...' with model llama2\n",
      "2025-09-28 14:44:25,271 - __main__ - WARNING - Classification failed for review: ''With YouTube playing while screens locked'...' with model llama2 using prompt Zero_Shot_Prompt\n",
      "Classifying reviews with llama2 (Zero_Shot_Prompt):  90%|███████████████████████▍  | 461/512 [1:34:47<40:12, 47.30s/it]2025-09-28 14:46:29,871 - __main__ - ERROR - Ollama request timed out for review: ''With iOS allowing content blocking in Safari and ...' with model llama2\n",
      "2025-09-28 14:46:30,056 - __main__ - WARNING - Classification failed for review: ''With iOS allowing content blocking in Safari and ...' with model llama2 using prompt Zero_Shot_Prompt\n",
      "Classifying reviews with llama2 (Zero_Shot_Prompt): 100%|██████████████████████████| 512/512 [1:53:03<00:00, 13.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama2 using Zero_Shot_Prompt completed in 113.06 minutes\n",
      "\n",
      "--- Sample of Predictions for llama2 (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0  feature request  \n",
      "1  feature request  \n",
      "2  feature request  \n",
      "3  feature request  \n",
      "4  feature request  \n",
      "\n",
      "--- Classification Report for llama2 (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.10      0.96      0.18        50\n",
      "     bug report       0.00      0.00      0.00       288\n",
      "          other       0.71      0.12      0.20       172\n",
      "\n",
      "       accuracy                           0.13       510\n",
      "      macro avg       0.27      0.36      0.13       510\n",
      "   weighted avg       0.25      0.13      0.09       510\n",
      "\n",
      "\n",
      "========== Evaluation for llama2 (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral (Zero_Shot_Prompt):  17%|████▎                     | 85/512 [35:44<4:21:10, 36.70s/it]2025-09-28 15:40:39,398 - __main__ - ERROR - Ollama request timed out for review: ''will not load this sucks try again '...' with model mistral\n",
      "2025-09-28 15:40:39,414 - __main__ - WARNING - Classification failed for review: ''will not load this sucks try again '...' with model mistral using prompt Zero_Shot_Prompt\n",
      "Classifying reviews with mistral (Zero_Shot_Prompt): 100%|█████████████████████████| 512/512 [2:18:10<00:00, 16.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with mistral using Zero_Shot_Prompt completed in 138.18 minutes\n",
      "\n",
      "--- Sample of Predictions for mistral (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report       other\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.26      0.70      0.38        50\n",
      "     bug report       0.91      0.48      0.63       287\n",
      "          other       0.61      0.79      0.69       174\n",
      "\n",
      "       accuracy                           0.61       511\n",
      "      macro avg       0.59      0.66      0.57       511\n",
      "   weighted avg       0.74      0.61      0.63       511\n",
      "\n",
      "\n",
      "========== Evaluation for mistral (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b (Zero_Shot_Prompt): 100%|███████████████████████| 512/512 [2:53:53<00:00, 20.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama3:8b using Zero_Shot_Prompt completed in 173.89 minutes\n",
      "\n",
      "--- Sample of Predictions for llama3:8b (Zero_Shot_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report       other\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Zero_Shot_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.78      0.36      0.49        50\n",
      "     bug report       0.79      0.91      0.84       288\n",
      "          other       0.77      0.70      0.73       174\n",
      "\n",
      "       accuracy                           0.78       512\n",
      "      macro avg       0.78      0.66      0.69       512\n",
      "   weighted avg       0.78      0.78      0.77       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama3:8b (Zero_Shot_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b (Zero_Shot_Prompt):  11%|██▊                      | 58/512 [30:04<4:14:48, 33.68s/it]2025-09-28 21:17:10,306 - __main__ - ERROR - Ollama request timed out for review: ''keeps force closing all the time'...' with model gemma:7b\n",
      "2025-09-28 21:17:10,331 - __main__ - WARNING - Classification failed for review: ''keeps force closing all the time'...' with model gemma:7b using prompt Zero_Shot_Prompt\n",
      "Classifying reviews with gemma:7b (Zero_Shot_Prompt):  56%|████████████▎         | 286/512 [3:07:25<1:57:52, 31.30s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Zero_Shot_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis for user feedback. Classify the given app review into exactly ONE of:\n",
    "- Feature Request\n",
    "- Bug Report\n",
    "- Other\n",
    "\n",
    "DEFINITIONS (concise):\n",
    "- Feature Request: Asks for a new capability or an enhancement to something that currently works.\n",
    "- Bug Report: Describes an error, crash, failure, or behavior not working as intended.\n",
    "- Other: Vague praise/complaint, questions, off-topic, or not enough info to decide.\n",
    "\n",
    "TIE-BREAK RULES:\n",
    "- If any error/failure is stated or implied → Bug Report.\n",
    "- If any explicit request/suggestion for new/improved functionality and no error → Feature Request.\n",
    "- Otherwise → Other.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return ONLY one of these exact strings with no extra text or punctuation:\n",
    "Feature Request | Bug Report | Other\n",
    "\n",
    "App Review Segment: '''{review_text}'''\n",
    "Answer:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e795a64-1a25-4cb6-a937-3e5369a71e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Zero_Shot_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis for user feedback. Classify the given app review into exactly ONE of:\n",
    "- Feature Request\n",
    "- Bug Report\n",
    "- Other\n",
    "\n",
    "DEFINITIONS (concise):\n",
    "- Feature Request: Asks for a new capability or an enhancement to something that currently works.\n",
    "- Bug Report: Describes an error, crash, failure, or behavior not working as intended.\n",
    "- Other: Vague praise/complaint, questions, off-topic, or not enough info to decide.\n",
    "\n",
    "TIE-BREAK RULES:\n",
    "- If any error/failure is stated or implied → Bug Report.\n",
    "- If any explicit request/suggestion for new/improved functionality and no error → Feature Request.\n",
    "- Otherwise → Other.\n",
    "\n",
    "OUTPUT FORMAT:\n",
    "Return ONLY one of these exact strings with no extra text or punctuation:\n",
    "Feature Request | Bug Report | Other\n",
    "\n",
    "App Review Segment: '''{review_text}'''\n",
    "Answer:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe6ed1-3432-4161-91e8-dc6fca3e9fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
