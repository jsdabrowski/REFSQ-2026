{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b54d4f21-b47e-48b2-9bdd-6123199110e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 512 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Chain_of_Thought_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2 (Chain_of_Thought_Prompt): 100%|█████████████████████| 512/512 [26:04<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama2 using Chain_of_Thought_Prompt completed in 26.07 minutes\n",
      "\n",
      "--- Sample of Predictions for llama2 (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama2 (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.22      0.20      0.21        50\n",
      "     bug report       0.63      0.99      0.77       288\n",
      "          other       0.93      0.08      0.15       174\n",
      "\n",
      "       accuracy                           0.60       512\n",
      "      macro avg       0.60      0.42      0.38       512\n",
      "   weighted avg       0.69      0.60      0.50       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama2 (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral (Chain_of_Thought_Prompt): 100%|██████████████████| 512/512 [1:02:38<00:00,  7.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with mistral using Chain_of_Thought_Prompt completed in 62.64 minutes\n",
      "\n",
      "--- Sample of Predictions for mistral (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report       other\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.37      0.78      0.50        50\n",
      "     bug report       0.83      0.80      0.82       288\n",
      "          other       0.76      0.57      0.65       174\n",
      "\n",
      "       accuracy                           0.72       512\n",
      "      macro avg       0.65      0.72      0.66       512\n",
      "   weighted avg       0.76      0.72      0.73       512\n",
      "\n",
      "\n",
      "========== Evaluation for mistral (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b (Chain_of_Thought_Prompt): 100%|████████████████| 512/512 [1:48:48<00:00, 12.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama3:8b using Chain_of_Thought_Prompt completed in 108.81 minutes\n",
      "\n",
      "--- Sample of Predictions for llama3:8b (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.55      0.58      0.56        50\n",
      "     bug report       0.80      0.87      0.84       288\n",
      "          other       0.77      0.65      0.71       174\n",
      "\n",
      "       accuracy                           0.77       512\n",
      "      macro avg       0.71      0.70      0.70       512\n",
      "   weighted avg       0.77      0.77      0.76       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama3:8b (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b (Chain_of_Thought_Prompt): 100%|█████████████████| 512/512 [1:22:26<00:00,  9.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with gemma:7b using Chain_of_Thought_Prompt completed in 82.45 minutes\n",
      "\n",
      "--- Sample of Predictions for gemma:7b (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0       bug report  \n",
      "1  feature request  \n",
      "2       bug report  \n",
      "3       bug report  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for gemma:7b (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.20      0.86      0.32        50\n",
      "     bug report       0.85      0.78      0.81       288\n",
      "          other       0.90      0.16      0.26       174\n",
      "\n",
      "       accuracy                           0.57       512\n",
      "      macro avg       0.65      0.60      0.47       512\n",
      "   weighted avg       0.80      0.57      0.58       512\n",
      "\n",
      "\n",
      "========== Evaluation for gemma:7b (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: phi3:mini ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with phi3:mini (Chain_of_Thought_Prompt):  90%|████████████████▏ | 459/512 [35:58<04:17,  4.86s/it]2025-09-17 12:35:34,628 - __main__ - WARNING - \n",
      "Failed Parsing for: ''Why am I getting ads that pop up''\n",
      "Raw LLM Output: '1. The review segment mentions receiving unwanted advertisements, which implies a negative experience with the app's current state of affairs regarding in-app purchases or monetization methods through ads. This does not suggest that there is an error within the functionality itself but rather indicates dissatisfaction and possibly misalignment between user expectations (e.g., no advertisements) and what they are currently experien0bting, which could be a feature of the app or its settings/preferences as designed by developers.\n",
      "\n",
      "2. Since there is an indication that something within the current functionality isn't meeting users’ desires for their experience (i.e., not wanting ads), and no explicit mention of bugs like crashes, incorrect behavior due to a feature malfunctioning or errors in code execution related directly to these pop-up advertisements being presented as an issue with existing features working incorrectly would be misleading without further context suggesting that the app is supposedly free from such intrusive ads.\n",
      "\n",
      "3. The feedback seems more like a user's expectation for how they should experience the application, which isn’t currently met by its design or settings regarding advertisements and in-app purchases/'\n",
      "Classifying reviews with phi3:mini (Chain_of_Thought_Prompt): 100%|██████████████████| 512/512 [39:59<00:00,  4.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with phi3:mini using Chain_of_Thought_Prompt completed in 39.99 minutes\n",
      "\n",
      "--- Sample of Predictions for phi3:mini (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0       bug report  \n",
      "1  feature request  \n",
      "2       bug report  \n",
      "3       bug report  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for phi3:mini (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.38      0.68      0.49        50\n",
      "     bug report       0.82      0.83      0.82       288\n",
      "          other       0.81      0.60      0.69       173\n",
      "\n",
      "       accuracy                           0.74       511\n",
      "      macro avg       0.67      0.70      0.67       511\n",
      "   weighted avg       0.77      0.74      0.75       511\n",
      "\n",
      "\n",
      "========== Evaluation for phi3:mini (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "\n",
      "========== ALL EVALUATION COMPLETE ==========\n",
      "\n",
      "Summary of Accuracies across all Prompt Strategies and Models:\n",
      "\n",
      "--- Prompt Strategy: Chain_of_Thought_Prompt ---\n",
      "  llama2: Accuracy = 0.60\n",
      "  mistral: Accuracy = 0.72\n",
      "  llama3:8b: Accuracy = 0.77\n",
      "  gemma:7b: Accuracy = 0.57\n",
      "  phi3:mini: Accuracy = 0.74\n",
      "\n",
      "--- Final Evaluation End ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Chain_of_Thought_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Think step-by-step about which category (Feature Request, Bug Report, Other) the review segment best fits, based on the definitions. Explain your reasoning.\n",
    "3.  After your step-by-step thinking process, state your final classification clearly, preceded by \"FINAL CLASSIFICATION:\".\n",
    "4.  Your final output for classification MUST be only the category name, without any additional text, explanation, or punctuation after \"FINAL CLASSIFICATION:\".\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Thinking Process:**\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e26462-ec59-487a-b01f-65a7ae98faf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 512 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Chain_of_Thought_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2 (Chain_of_Thought_Prompt): 100%|█████████████████████| 512/512 [27:12<00:00,  3.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama2 using Chain_of_Thought_Prompt completed in 27.21 minutes\n",
      "\n",
      "--- Sample of Predictions for llama2 (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama2 (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.21      0.20      0.21        50\n",
      "     bug report       0.63      0.99      0.77       288\n",
      "          other       0.93      0.08      0.15       174\n",
      "\n",
      "       accuracy                           0.60       512\n",
      "      macro avg       0.59      0.42      0.38       512\n",
      "   weighted avg       0.69      0.60      0.50       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama2 (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral (Chain_of_Thought_Prompt): 100%|██████████████████| 512/512 [1:28:58<00:00, 10.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with mistral using Chain_of_Thought_Prompt completed in 88.98 minutes\n",
      "\n",
      "--- Sample of Predictions for mistral (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report       other\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.38      0.80      0.52        50\n",
      "     bug report       0.83      0.77      0.80       288\n",
      "          other       0.75      0.60      0.67       174\n",
      "\n",
      "       accuracy                           0.72       512\n",
      "      macro avg       0.66      0.73      0.66       512\n",
      "   weighted avg       0.76      0.72      0.73       512\n",
      "\n",
      "\n",
      "========== Evaluation for mistral (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b (Chain_of_Thought_Prompt): 100%|████████████████| 512/512 [2:13:52<00:00, 15.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama3:8b using Chain_of_Thought_Prompt completed in 133.87 minutes\n",
      "\n",
      "--- Sample of Predictions for llama3:8b (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report       other\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.48      0.58      0.52        50\n",
      "     bug report       0.79      0.82      0.81       288\n",
      "          other       0.72      0.63      0.67       174\n",
      "\n",
      "       accuracy                           0.73       512\n",
      "      macro avg       0.66      0.68      0.67       512\n",
      "   weighted avg       0.74      0.73      0.73       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama3:8b (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b (Chain_of_Thought_Prompt):  44%|██████▋        | 227/512 [1:04:31<1:19:36, 16.76s/it]2025-09-19 06:40:30,011 - __main__ - ERROR - HTTP error occurred: 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate - {\"error\":\"llama runner process has terminated: cudaMalloc failed: out of memory\"} with model gemma:7b\n",
      "2025-09-19 06:40:30,021 - __main__ - WARNING - Classification failed for review: ''A ridiculous amount of RAM'...' with model gemma:7b using prompt Chain_of_Thought_Prompt\n",
      "Classifying reviews with gemma:7b (Chain_of_Thought_Prompt): 100%|█████████████████| 512/512 [2:04:16<00:00, 14.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with gemma:7b using Chain_of_Thought_Prompt completed in 124.28 minutes\n",
      "\n",
      "--- Sample of Predictions for gemma:7b (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0       bug report  \n",
      "1  feature request  \n",
      "2       bug report  \n",
      "3       bug report  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for gemma:7b (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.19      0.86      0.31        50\n",
      "     bug report       0.85      0.77      0.81       287\n",
      "          other       0.92      0.13      0.22       174\n",
      "\n",
      "       accuracy                           0.56       511\n",
      "      macro avg       0.65      0.59      0.45       511\n",
      "   weighted avg       0.81      0.56      0.56       511\n",
      "\n",
      "\n",
      "========== Evaluation for gemma:7b (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: phi3:mini ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with phi3:mini (Chain_of_Thought_Prompt):  80%|████████████▉   | 412/512 [1:06:51<06:41,  4.02s/it]2025-09-19 08:46:51,224 - __main__ - WARNING - \n",
      "Failed Parsing for: ''It feels bad reseting 2000 custom words''\n",
      "Raw LLM Output: '1. The review segment mentions a negative experience with the app's functionality, specifically regarding saving and retrieving data (custom words). This indicates that there is an issue related to how personal information or user-generated content within the application should be handled by the software system. It does not suggest adding new features but rather improving existing ones for better performance in terms of preserving custom settings without loss upon resetting, which implies a flaw with data handling mechanisms that could lead users' work to disappear unexpectedly when they perform certain actions (like reset).\n",
      "\n",
      "2. The feedback is pointing out an unintended behavior where the app does not retain user-generated content after performing what seems like routine maintenance or cleanup tasks, which should be a standard feature without causing data loss for personalized settings that users have created over time within their account on this platform (assuming it's some sort of writing tool).\n",
      "\n",
      "3. The review is clearly identifying an issue with the app’s functionality and not suggesting any new features or improvements unrelated to bugs, nor does it seem like a general comment without specific feedback about software behavior that could be categorized as irrelevant information (such as asking for help on how to use certain non-bug related functional'\n",
      "Classifying reviews with phi3:mini (Chain_of_Thought_Prompt): 100%|████████████████| 512/512 [1:14:38<00:00,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with phi3:mini using Chain_of_Thought_Prompt completed in 74.64 minutes\n",
      "\n",
      "--- Sample of Predictions for phi3:mini (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for phi3:mini (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.37      0.60      0.45        50\n",
      "     bug report       0.80      0.82      0.81       288\n",
      "          other       0.80      0.62      0.70       173\n",
      "\n",
      "       accuracy                           0.73       511\n",
      "      macro avg       0.66      0.68      0.66       511\n",
      "   weighted avg       0.76      0.73      0.74       511\n",
      "\n",
      "\n",
      "========== Evaluation for phi3:mini (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "\n",
      "========== ALL EVALUATION COMPLETE ==========\n",
      "\n",
      "Summary of Accuracies across all Prompt Strategies and Models:\n",
      "\n",
      "--- Prompt Strategy: Chain_of_Thought_Prompt ---\n",
      "  llama2: Accuracy = 0.60\n",
      "  mistral: Accuracy = 0.72\n",
      "  llama3:8b: Accuracy = 0.73\n",
      "  gemma:7b: Accuracy = 0.56\n",
      "  phi3:mini: Accuracy = 0.73\n",
      "\n",
      "--- Final Evaluation End ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Chain_of_Thought_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Think step-by-step about which category (Feature Request, Bug Report, Other) the review segment best fits, based on the definitions. Explain your reasoning.\n",
    "3.  After your step-by-step thinking process, state your final classification clearly, preceded by \"FINAL CLASSIFICATION:\".\n",
    "4.  Your final output for classification MUST be only the category name, without any additional text, explanation, or punctuation after \"FINAL CLASSIFICATION:\".\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Thinking Process:**\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afeff39-a5f9-4b07-a19d-85007837d781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 512 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Chain_of_Thought_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2 (Chain_of_Thought_Prompt): 100%|█████████████████████| 512/512 [35:20<00:00,  4.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama2 using Chain_of_Thought_Prompt completed in 35.34 minutes\n",
      "\n",
      "--- Sample of Predictions for llama2 (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama2 (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.21      0.20      0.21        50\n",
      "     bug report       0.64      0.99      0.78       288\n",
      "          other       0.94      0.10      0.18       174\n",
      "\n",
      "       accuracy                           0.61       512\n",
      "      macro avg       0.60      0.43      0.39       512\n",
      "   weighted avg       0.70      0.61      0.52       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama2 (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral (Chain_of_Thought_Prompt):  17%|███▎               | 88/512 [16:56<1:25:13, 12.06s/it]2025-09-19 09:48:51,740 - __main__ - ERROR - Ollama request timed out for review: ''it worked perfectly fine for me until yesturday w...' with model mistral\n",
      "2025-09-19 09:48:51,965 - __main__ - WARNING - Classification failed for review: ''it worked perfectly fine for me until yesturday w...' with model mistral using prompt Chain_of_Thought_Prompt\n",
      "Classifying reviews with mistral (Chain_of_Thought_Prompt): 100%|██████████████████| 512/512 [1:35:01<00:00, 11.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with mistral using Chain_of_Thought_Prompt completed in 95.02 minutes\n",
      "\n",
      "--- Sample of Predictions for mistral (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report       other\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.39      0.84      0.54        50\n",
      "     bug report       0.83      0.76      0.79       287\n",
      "          other       0.72      0.58      0.64       174\n",
      "\n",
      "       accuracy                           0.70       511\n",
      "      macro avg       0.64      0.73      0.66       511\n",
      "   weighted avg       0.75      0.70      0.71       511\n",
      "\n",
      "\n",
      "========== Evaluation for mistral (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b (Chain_of_Thought_Prompt): 100%|████████████████| 512/512 [2:14:54<00:00, 15.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama3:8b using Chain_of_Thought_Prompt completed in 134.90 minutes\n",
      "\n",
      "--- Sample of Predictions for llama3:8b (Chain_of_Thought_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Chain_of_Thought_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.48      0.54      0.51        50\n",
      "     bug report       0.81      0.85      0.83       288\n",
      "          other       0.76      0.67      0.71       174\n",
      "\n",
      "       accuracy                           0.76       512\n",
      "      macro avg       0.68      0.68      0.68       512\n",
      "   weighted avg       0.76      0.76      0.76       512\n",
      "\n",
      "\n",
      "========== Evaluation for llama3:8b (Chain_of_Thought_Prompt) Complete ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b (Chain_of_Thought_Prompt):  39%|█████▊         | 198/512 [1:08:59<2:11:38, 25.15s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Chain_of_Thought_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Think step-by-step about which category (Feature Request, Bug Report, Other) the review segment best fits, based on the definitions. Explain your reasoning.\n",
    "3.  After your step-by-step thinking process, state your final classification clearly, preceded by \"FINAL CLASSIFICATION:\".\n",
    "4.  Your final output for classification MUST be only the category name, without any additional text, explanation, or punctuation after \"FINAL CLASSIFICATION:\".\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Thinking Process:**\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28c396-f4b6-4a6a-b624-cd7e838085a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Define Prompting Strategies (Now only includes Chain-of-Thought) ---\n",
    "\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Chain_of_Thought_Prompt\": \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Think step-by-step about which category (Feature Request, Bug Report, Other) the review segment best fits, based on the definitions. Explain your reasoning.\n",
    "3.  After your step-by-step thinking process, state your final classification clearly, preceded by \"FINAL CLASSIFICATION:\".\n",
    "4.  Your final output for classification MUST be only the category name, without any additional text, explanation, or punctuation after \"FINAL CLASSIFICATION:\".\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Thinking Process:**\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model using a specified prompt template.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = prompt_template.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 256 # Increase num_predict for CoT to allow for more output\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models and Prompt Strategies ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        \n",
    "        predictions = []\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name} ({prompt_name})\"):\n",
    "            response_data = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            \n",
    "            if response_data[\"success\"]:\n",
    "                predicted_raw = response_data[\"raw_response\"].strip()\n",
    "                \n",
    "                # Regex specifically for Chain-of-Thought, looking for \"FINAL CLASSIFICATION:\"\n",
    "                # This makes parsing more robust when the LLM provides reasoning first.\n",
    "                match = re.search(\n",
    "                    r\"FINAL CLASSIFICATION:\\s*(feature request|bug report|other)\",\n",
    "                    predicted_raw,\n",
    "                    re.IGNORECASE | re.DOTALL\n",
    "                )\n",
    "                \n",
    "                if match:\n",
    "                    pred = match.group(1).strip().lower()\n",
    "                else:\n",
    "                    # Fallback: If \"FINAL CLASSIFICATION:\" isn't found, try to find any valid label\n",
    "                    # in the last few lines, as models might sometimes deviate.\n",
    "                    # This is less ideal for CoT but helps catch cases where the model\n",
    "                    # doesn't strictly follow the \"FINAL CLASSIFICATION\" format.\n",
    "                    lines = predicted_raw.split('\\n')\n",
    "                    found_valid_label = False\n",
    "                    for line in reversed(lines): # Check from bottom up\n",
    "                        for label in VALID_FR_LABELS:\n",
    "                            if label in line.lower():\n",
    "                                pred = label\n",
    "                                found_valid_label = True\n",
    "                                break\n",
    "                        if found_valid_label:\n",
    "                            break\n",
    "                    if not found_valid_label:\n",
    "                        pred = \"Failed Parsing\"\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name} using prompt {prompt_name}\")\n",
    "                \n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ Classification with {current_model_name} using {prompt_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        # --- 7. Prepare Results and Generate Classification Report for current model and prompt ---\n",
    "        results_df_current_run = fr_data.copy()\n",
    "        results_df_current_run['predicted'] = predictions\n",
    "\n",
    "        filtered_results = results_df_current_run[\n",
    "            (results_df_current_run['predicted'] != 'failed') &\n",
    "            (results_df_current_run['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample of Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df_current_run.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered_results.empty:\n",
    "            report = classification_report(\n",
    "                filtered_results['ground_truth'],\n",
    "                filtered_results['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "                'report': report # Store the full report string\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions to generate a classification report for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        \n",
    "        print(f\"\\n{'='*10} Evaluation for {current_model_name} ({prompt_name}) Complete {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies across all Prompt Strategies and Models:\")\n",
    "\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc908bc3-2655-4c08-9ade-d5328693ed23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
