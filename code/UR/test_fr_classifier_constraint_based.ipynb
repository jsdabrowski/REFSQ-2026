{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be04f77-28e3-4755-8fd7-b41a3302bd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test.txt (Full Dataset) ---\n",
      "Loaded 512 reviews.\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Constraint_Based_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with llama2 (Constraint_Based_Prompt): 100%|███████████████████████████| 512/512 [2:13:43<00:00, 15.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ llama2 using Constraint_Based_Prompt finished in 133.73 minutes\n",
      "\n",
      "--- Sample Predictions for llama2 (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0       bug report  \n",
      "1            other  \n",
      "2  feature request  \n",
      "3       bug report  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for llama2 (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.18      0.74      0.29        50\n",
      "     bug report       0.84      0.48      0.61       288\n",
      "          other       0.58      0.47      0.52       174\n",
      "\n",
      "       accuracy                           0.50       512\n",
      "      macro avg       0.53      0.56      0.47       512\n",
      "   weighted avg       0.69      0.50      0.55       512\n",
      "\n",
      "\n",
      "========== Done: llama2 (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with mistral (Constraint_Based_Prompt): 100%|██████████████████████████| 512/512 [2:00:28<00:00, 14.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mistral using Constraint_Based_Prompt finished in 120.48 minutes\n",
      "\n",
      "--- Sample Predictions for mistral (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.34      0.84      0.49        50\n",
      "     bug report       0.78      0.89      0.83       288\n",
      "          other       0.98      0.33      0.49       174\n",
      "\n",
      "       accuracy                           0.70       512\n",
      "      macro avg       0.70      0.69      0.60       512\n",
      "   weighted avg       0.80      0.70      0.68       512\n",
      "\n",
      "\n",
      "========== Done: mistral (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with llama3:8b (Constraint_Based_Prompt): 100%|████████████████████████| 512/512 [2:36:38<00:00, 18.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ llama3:8b using Constraint_Based_Prompt finished in 156.64 minutes\n",
      "\n",
      "--- Sample Predictions for llama3:8b (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report       other\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.24      0.80      0.37        50\n",
      "     bug report       0.81      0.82      0.81       288\n",
      "          other       0.87      0.26      0.41       174\n",
      "\n",
      "       accuracy                           0.63       512\n",
      "      macro avg       0.64      0.63      0.53       512\n",
      "   weighted avg       0.77      0.63      0.63       512\n",
      "\n",
      "\n",
      "========== Done: llama3:8b (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with gemma:7b (Constraint_Based_Prompt): 100%|█████████████████████████| 512/512 [4:17:56<00:00, 30.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ gemma:7b using Constraint_Based_Prompt finished in 257.94 minutes\n",
      "\n",
      "--- Sample Predictions for gemma:7b (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for gemma:7b (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.39      0.68      0.49        50\n",
      "     bug report       0.73      0.94      0.82       288\n",
      "          other       1.00      0.29      0.45       174\n",
      "\n",
      "       accuracy                           0.70       512\n",
      "      macro avg       0.71      0.64      0.59       512\n",
      "   weighted avg       0.79      0.70      0.67       512\n",
      "\n",
      "\n",
      "========== Done: gemma:7b (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: phi3:mini ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with phi3:mini (Constraint_Based_Prompt): 100%|██████████████████████████| 512/512 [59:28<00:00,  6.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ phi3:mini using Constraint_Based_Prompt finished in 59.47 minutes\n",
      "\n",
      "--- Sample Predictions for phi3:mini (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report       other\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for phi3:mini (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.37      0.80      0.51        50\n",
      "     bug report       0.90      0.69      0.78       286\n",
      "          other       0.66      0.69      0.68       174\n",
      "\n",
      "       accuracy                           0.70       510\n",
      "      macro avg       0.64      0.73      0.65       510\n",
      "   weighted avg       0.76      0.70      0.72       510\n",
      "\n",
      "\n",
      "========== Done: phi3:mini (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "\n",
      "========== ALL EVALUATION COMPLETE ==========\n",
      "\n",
      "Summary of Accuracies:\n",
      "\n",
      "--- Prompt Strategy: Constraint_Based_Prompt ---\n",
      "  llama2: Accuracy = 0.50\n",
      "  mistral: Accuracy = 0.70\n",
      "  llama3:8b: Accuracy = 0.63\n",
      "  gemma:7b: Accuracy = 0.70\n",
      "  phi3:mini: Accuracy = 0.70\n",
      "\n",
      "--- Final Evaluation End ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from string import Template\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\"llama2\", \"mistral\", \"llama3:8b\", \"gemma:7b\", \"phi3:mini\"]\n",
    "\n",
    "# --- 3. Data Loading ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "label_mapping = {'bugreport': 'bug report', 'featurerequest': 'feature request', 'other': 'other'}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} invalid rows.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} reviews.\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Prompt (Constraint-Based) ---\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Constraint_Based_Prompt\": \"\"\"\n",
    "You are an expert classifier. Enforce the constraints below strictly.\n",
    "\n",
    "TASK\n",
    "Classify the app review into exactly ONE category:\n",
    "- Feature Request\n",
    "- Bug Report\n",
    "- Other\n",
    "\n",
    "CONSTRAINTS\n",
    "1) Output MUST be valid JSON, no preface/suffix text.\n",
    "2) \"category\" MUST be one of: [\"Feature Request\",\"Bug Report\",\"Other\"].\n",
    "3) \"evidence\" MUST be an exact substring from the review (<=140 chars) justifying the label.\n",
    "4) \"justification\" MUST be <= 20 words, no numbered steps or hidden reasoning.\n",
    "5) \"confidence\" is a float in [0,1] (use 0.50, 0.75, 0.90, 0.95, or 0.99).\n",
    "6) If error/failure implied → \"Bug Report\". If explicit ask for new/improved functionality without error → \"Feature Request\". Else → \"Other\".\n",
    "\n",
    "OUTPUT JSON SCHEMA\n",
    "{\n",
    "  \"category\": \"Feature Request|Bug Report|Other\",\n",
    "  \"evidence\": \"substring from review\",\n",
    "  \"justification\": \"≤20 words\",\n",
    "  \"confidence\": 0.50\n",
    "}\n",
    "\n",
    "REVIEW: '''${review_text}'''\n",
    "ONLY RETURN THE JSON OBJECT:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- Helpers ---\n",
    "def format_prompt(template_str: str, review_text: str) -> str:\n",
    "    return Template(template_str).substitute(review_text=review_text)\n",
    "\n",
    "def extract_json_object(text: str):\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Find first JSON object\n",
    "    m = re.search(r'\\{.*\\}', text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    snippet = m.group(0)\n",
    "    for candidate in (snippet, re.sub(r\"'\", '\"', snippet)):  # fix single quotes if needed\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def normalize_category(cat: str) -> str:\n",
    "    if not isinstance(cat, str):\n",
    "        return \"Failed Parsing\"\n",
    "    cat_l = cat.strip().lower()\n",
    "    if cat_l in VALID_FR_LABELS:\n",
    "        return cat_l\n",
    "    # Title-case variants\n",
    "    mapping = {\n",
    "        \"feature request\": \"feature request\",\n",
    "        \"bug report\": \"bug report\",\n",
    "        \"other\": \"other\"\n",
    "    }\n",
    "    return mapping.get(cat_l, \"Failed Parsing\")\n",
    "\n",
    "# --- 5. LLM Call ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    prompt = format_prompt(prompt_template, review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.0, \"num_predict\": 256}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        r.raise_for_status()\n",
    "        return {\"success\": True, \"raw_response\": r.json().get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama at {OLLAMA_BASE_URL}. Is it running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error: {http_err} - {r.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        predictions, start_time = [], time.time()\n",
    "\n",
    "        for _, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying with {current_model_name} ({prompt_name})\"):\n",
    "            resp = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            if resp[\"success\"]:\n",
    "                raw = resp[\"raw_response\"].strip()\n",
    "                obj = extract_json_object(raw)\n",
    "                if obj and isinstance(obj, dict) and \"category\" in obj:\n",
    "                    pred = normalize_category(obj[\"category\"])\n",
    "                else:\n",
    "                    # last-ditch: scan for label tokens\n",
    "                    m = re.search(r\"\\b(feature request|bug report|other)\\b\", raw, re.IGNORECASE)\n",
    "                    pred = m.group(1).strip().lower() if m else \"Failed Parsing\"\n",
    "                    if pred == \"Failed Parsing\":\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw: '{raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with {current_model_name}\")\n",
    "\n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ {current_model_name} using {prompt_name} finished in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        results_df = fr_data.copy()\n",
    "        results_df['predicted'] = predictions\n",
    "\n",
    "        filtered = results_df[\n",
    "            (results_df['predicted'] != 'failed') &\n",
    "            (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered.empty:\n",
    "            report = classification_report(\n",
    "                filtered['ground_truth'],\n",
    "                filtered['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered['ground_truth'], filtered['predicted']),\n",
    "                'report': report\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        print(f\"\\n{'='*10} Done: {current_model_name} ({prompt_name}) {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies:\")\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "print(\"\\n--- Final Evaluation End ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4dd61a-77e6-4987-96a1-c66a8fd94599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test.txt (Full Dataset) ---\n",
      "Loaded 512 reviews.\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "\n",
      "=============== Evaluating with Prompt Strategy: Constraint_Based_Prompt ===============\n",
      "\n",
      "========== Starting Classification for Model: llama2 ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with llama2 (Constraint_Based_Prompt):  43%|███████████▋               | 221/512 [52:48<1:10:03, 14.45s/it]2025-09-28 14:44:23,662 - __main__ - ERROR - Timeout for review: ''Great app but becomes sluggish every now and then...' with model llama2\n",
      "2025-09-28 14:44:23,788 - __main__ - WARNING - Classification failed for review: ''Great app but becomes sluggish every now and then...' with llama2\n",
      "Classifying with llama2 (Constraint_Based_Prompt):  43%|███████████▋               | 222/512 [54:51<3:47:21, 47.04s/it]2025-09-28 14:46:26,915 - __main__ - ERROR - Timeout for review: ''Its not a bad keyboard replacement but it insists...' with model llama2\n",
      "2025-09-28 14:46:28,540 - __main__ - WARNING - Classification failed for review: ''Its not a bad keyboard replacement but it insists...' with llama2\n",
      "Classifying with llama2 (Constraint_Based_Prompt):  44%|███████████▊               | 223/512 [56:55<5:38:45, 70.33s/it]2025-09-28 14:48:41,037 - __main__ - ERROR - Timeout for review: ''But the predictions bar doesnt appear for for the...' with model llama2\n",
      "2025-09-28 14:48:41,039 - __main__ - WARNING - Classification failed for review: ''But the predictions bar doesnt appear for for the...' with llama2\n",
      "Classifying with llama2 (Constraint_Based_Prompt):  70%|█████████████████▍       | 358/512 [1:48:48<1:38:01, 38.19s/it]2025-09-28 15:40:39,399 - __main__ - ERROR - Timeout for review: ''i want my 20 fish bucks back'...' with model llama2\n",
      "2025-09-28 15:40:39,438 - __main__ - WARNING - Classification failed for review: ''i want my 20 fish bucks back'...' with llama2\n",
      "Classifying with llama2 (Constraint_Based_Prompt): 100%|███████████████████████████| 512/512 [2:48:11<00:00, 19.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ llama2 using Constraint_Based_Prompt finished in 168.20 minutes\n",
      "\n",
      "--- Sample Predictions for llama2 (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth  \\\n",
      "0                'this version crashes all the time'   bug report   \n",
      "1                    'it take a lot time in loading'   bug report   \n",
      "2                               'pages freeze often'   bug report   \n",
      "3  'still having problems uploading sometimes tho...   bug report   \n",
      "4  'it wont load any of my notifications when i c...   bug report   \n",
      "\n",
      "         predicted  \n",
      "0       bug report  \n",
      "1            other  \n",
      "2  feature request  \n",
      "3       bug report  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for llama2 (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.18      0.72      0.28        50\n",
      "     bug report       0.85      0.49      0.63       285\n",
      "          other       0.62      0.49      0.55       173\n",
      "\n",
      "       accuracy                           0.52       508\n",
      "      macro avg       0.55      0.57      0.49       508\n",
      "   weighted avg       0.71      0.52      0.57       508\n",
      "\n",
      "\n",
      "========== Done: llama2 (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: mistral ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with mistral (Constraint_Based_Prompt): 100%|██████████████████████████| 512/512 [2:17:09<00:00, 16.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ mistral using Constraint_Based_Prompt finished in 137.17 minutes\n",
      "\n",
      "--- Sample Predictions for mistral (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.35      0.86      0.50        50\n",
      "     bug report       0.78      0.89      0.83       288\n",
      "          other       0.97      0.33      0.49       174\n",
      "\n",
      "       accuracy                           0.70       512\n",
      "      macro avg       0.70      0.69      0.61       512\n",
      "   weighted avg       0.80      0.70      0.68       512\n",
      "\n",
      "\n",
      "========== Done: mistral (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: llama3:8b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with llama3:8b (Constraint_Based_Prompt):  65%|██████████████▏       | 331/512 [1:49:29<1:33:23, 30.96s/it]2025-09-28 20:56:30,637 - __main__ - ERROR - Timeout for review: ''Great app but without emoji support its only sub ...' with model llama3:8b\n",
      "2025-09-28 20:56:30,718 - __main__ - WARNING - Classification failed for review: ''Great app but without emoji support its only sub ...' with llama3:8b\n",
      "Classifying with llama3:8b (Constraint_Based_Prompt): 100%|████████████████████████| 512/512 [4:02:31<00:00, 28.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ llama3:8b using Constraint_Based_Prompt finished in 242.52 minutes\n",
      "\n",
      "--- Sample Predictions for llama3:8b (Constraint_Based_Prompt) ---\n",
      "                                              review ground_truth   predicted\n",
      "0                'this version crashes all the time'   bug report  bug report\n",
      "1                    'it take a lot time in loading'   bug report  bug report\n",
      "2                               'pages freeze often'   bug report  bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report  bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b (Constraint_Based_Prompt) ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.23      0.80      0.36        49\n",
      "     bug report       0.81      0.83      0.82       288\n",
      "          other       0.88      0.25      0.39       174\n",
      "\n",
      "       accuracy                           0.63       511\n",
      "      macro avg       0.64      0.63      0.53       511\n",
      "   weighted avg       0.78      0.63      0.63       511\n",
      "\n",
      "\n",
      "========== Done: llama3:8b (Constraint_Based_Prompt) ==========\n",
      "\n",
      "\n",
      "========== Starting Classification for Model: gemma:7b ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with gemma:7b (Constraint_Based_Prompt):   9%|██▍                       | 47/512 [24:42<4:05:14, 31.65s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from string import Template\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\"llama2\", \"mistral\", \"llama3:8b\", \"gemma:7b\", \"phi3:mini\"]\n",
    "\n",
    "# --- 3. Data Loading ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "label_mapping = {'bugreport': 'bug report', 'featurerequest': 'feature request', 'other': 'other'}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} invalid rows.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} reviews.\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. Prompt (Constraint-Based) ---\n",
    "PROMPT_STRATEGIES = {\n",
    "    \"Constraint_Based_Prompt\": \"\"\"\n",
    "You are an expert classifier. Enforce the constraints below strictly.\n",
    "\n",
    "TASK\n",
    "Classify the app review into exactly ONE category:\n",
    "- Feature Request\n",
    "- Bug Report\n",
    "- Other\n",
    "\n",
    "CONSTRAINTS\n",
    "1) Output MUST be valid JSON, no preface/suffix text.\n",
    "2) \"category\" MUST be one of: [\"Feature Request\",\"Bug Report\",\"Other\"].\n",
    "3) \"evidence\" MUST be an exact substring from the review (<=140 chars) justifying the label.\n",
    "4) \"justification\" MUST be <= 20 words, no numbered steps or hidden reasoning.\n",
    "5) \"confidence\" is a float in [0,1] (use 0.50, 0.75, 0.90, 0.95, or 0.99).\n",
    "6) If error/failure implied → \"Bug Report\". If explicit ask for new/improved functionality without error → \"Feature Request\". Else → \"Other\".\n",
    "\n",
    "OUTPUT JSON SCHEMA\n",
    "{\n",
    "  \"category\": \"Feature Request|Bug Report|Other\",\n",
    "  \"evidence\": \"substring from review\",\n",
    "  \"justification\": \"≤20 words\",\n",
    "  \"confidence\": 0.50\n",
    "}\n",
    "\n",
    "REVIEW: '''${review_text}'''\n",
    "ONLY RETURN THE JSON OBJECT:\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# --- Helpers ---\n",
    "def format_prompt(template_str: str, review_text: str) -> str:\n",
    "    return Template(template_str).substitute(review_text=review_text)\n",
    "\n",
    "def extract_json_object(text: str):\n",
    "    # Try direct parse\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Find first JSON object\n",
    "    m = re.search(r'\\{.*\\}', text, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return None\n",
    "    snippet = m.group(0)\n",
    "    for candidate in (snippet, re.sub(r\"'\", '\"', snippet)):  # fix single quotes if needed\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def normalize_category(cat: str) -> str:\n",
    "    if not isinstance(cat, str):\n",
    "        return \"Failed Parsing\"\n",
    "    cat_l = cat.strip().lower()\n",
    "    if cat_l in VALID_FR_LABELS:\n",
    "        return cat_l\n",
    "    # Title-case variants\n",
    "    mapping = {\n",
    "        \"feature request\": \"feature request\",\n",
    "        \"bug report\": \"bug report\",\n",
    "        \"other\": \"other\"\n",
    "    }\n",
    "    return mapping.get(cat_l, \"Failed Parsing\")\n",
    "\n",
    "# --- 5. LLM Call ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str, prompt_template: str) -> dict:\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    prompt = format_prompt(prompt_template, review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\"temperature\": 0.0, \"num_predict\": 256}\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        r.raise_for_status()\n",
    "        return {\"success\": True, \"raw_response\": r.json().get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama at {OLLAMA_BASE_URL}. Is it running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Timeout for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error: {http_err} - {r.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation ---\n",
    "all_evaluation_results = {}\n",
    "\n",
    "for prompt_name, prompt_template in PROMPT_STRATEGIES.items():\n",
    "    print(f\"\\n\\n{'='*15} Evaluating with Prompt Strategy: {prompt_name} {'='*15}\")\n",
    "    all_evaluation_results[prompt_name] = {}\n",
    "\n",
    "    for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "        print(f\"\\n{'='*10} Starting Classification for Model: {current_model_name} {'='*10}\")\n",
    "        predictions, start_time = [], time.time()\n",
    "\n",
    "        for _, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying with {current_model_name} ({prompt_name})\"):\n",
    "            resp = classify_with_ollama_model(row['review'], current_model_name, prompt_template)\n",
    "            if resp[\"success\"]:\n",
    "                raw = resp[\"raw_response\"].strip()\n",
    "                obj = extract_json_object(raw)\n",
    "                if obj and isinstance(obj, dict) and \"category\" in obj:\n",
    "                    pred = normalize_category(obj[\"category\"])\n",
    "                else:\n",
    "                    # last-ditch: scan for label tokens\n",
    "                    m = re.search(r\"\\b(feature request|bug report|other)\\b\", raw, re.IGNORECASE)\n",
    "                    pred = m.group(1).strip().lower() if m else \"Failed Parsing\"\n",
    "                    if pred == \"Failed Parsing\":\n",
    "                        logger.warning(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw: '{raw}'\")\n",
    "            else:\n",
    "                pred = \"Failed\"\n",
    "                logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with {current_model_name}\")\n",
    "\n",
    "            predictions.append(pred)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"\\n✅ {current_model_name} using {prompt_name} finished in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "        results_df = fr_data.copy()\n",
    "        results_df['predicted'] = predictions\n",
    "\n",
    "        filtered = results_df[\n",
    "            (results_df['predicted'] != 'failed') &\n",
    "            (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "        ]\n",
    "\n",
    "        print(f\"\\n--- Sample Predictions for {current_model_name} ({prompt_name}) ---\")\n",
    "        print(results_df.head())\n",
    "\n",
    "        print(f\"\\n--- Classification Report for {current_model_name} ({prompt_name}) ---\")\n",
    "        if not filtered.empty:\n",
    "            report = classification_report(\n",
    "                filtered['ground_truth'],\n",
    "                filtered['predicted'],\n",
    "                labels=VALID_FR_LABELS,\n",
    "                zero_division=0\n",
    "            )\n",
    "            print(report)\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': accuracy_score(filtered['ground_truth'], filtered['predicted']),\n",
    "                'report': report\n",
    "            }\n",
    "        else:\n",
    "            print(f\"No valid predictions for {current_model_name} ({prompt_name}).\")\n",
    "            all_evaluation_results[prompt_name][current_model_name] = {\n",
    "                'accuracy': 0.0,\n",
    "                'report': \"No valid predictions.\"\n",
    "            }\n",
    "        print(f\"\\n{'='*10} Done: {current_model_name} ({prompt_name}) {'='*10}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies:\")\n",
    "for prompt_name, models_data in all_evaluation_results.items():\n",
    "    print(f\"\\n--- Prompt Strategy: {prompt_name} ---\")\n",
    "    for model, metrics in models_data.items():\n",
    "        print(f\"  {model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "print(\"\\n--- Final Evaluation End ---\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
