{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8768a284-6559-4732-b196-3dead6bf1440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51d6e6-ec1f-4f4c-bd5f-49d73affef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Set up basic logging for clarity in notebook output\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# --- CHANGE THIS LINE ---\n",
    "# Set the llm_client's logging level to INFO or WARNING to prevent excessive debug messages\n",
    "logging.getLogger('src.llm_client').setLevel(logging.INFO) # Or logging.WARNING for even less verbosity\n",
    "# --- END CHANGE ---\n",
    "\n",
    "# Adjust sys.path to ensure modules in 'src' can be imported\n",
    "if \"src\" not in sys.path:\n",
    "    sys.path.append(\"src\")\n",
    "\n",
    "from src.llm_client import LLMClient\n",
    "from src.config import Config\n",
    "\n",
    "# --- 1. Data Loading and Preparation for Functional Requirements ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test.txt ---\")\n",
    "# Load the data from BOW_test.txt\n",
    "# Assuming BOW_test.txt has 'review_text,classification' format\n",
    "fr_data = pd.read_csv(\n",
    "    \"datasets/BOW_test_sample.txt\",\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Handle potential malformed lines\n",
    ")\n",
    "\n",
    "# Standardize ground_truth labels to lowercase and strip whitespace\n",
    "fr_data['ground_truth'] = fr_data['ground_truth'].str.strip().str.lower()\n",
    "\n",
    "# Define valid FR categories for standardization and filtering later\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# Filter out any ground_truth labels that are not in our defined VALID_FR_LABELS\n",
    "initial_len = len(fr_data)\n",
    "fr_data = fr_data[fr_data['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown 'ground_truth' labels.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 2. LLM Client Initialization ---\n",
    "print(\"--- Initializing LLM Client ---\")\n",
    "client = LLMClient()\n",
    "if not client.test_connection():\n",
    "    print(\"❌ LLM connection failed at initialization. Please check API key in config, model name, and network.\")\n",
    "    # It's good practice to exit or raise an exception here if connection is mandatory\n",
    "else:\n",
    "    print(\"✅ LLM Client initialized and connected.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 3. Define FR Definitions & Few-Shot Examples (for FC2, FC3) ---\n",
    "# Definitions for FC3\n",
    "FR_DEFINITIONS = {\n",
    "    \"feature request\": \"A suggestion for new functionality, enhancements, or improvements to existing features.\",\n",
    "    \"bug report\": \"Describes an error, fault, or flaw in the system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways.\",\n",
    "    \"other\": \"The review does not clearly fall into Feature Request or Bug Report.\"\n",
    "}\n",
    "\n",
    "# Few-Shot Examples for FC2\n",
    "# IMPORTANT: Replace these with actual examples from your BOW_test.txt\n",
    "# if you have good, diverse examples that fit your categories.\n",
    "# These are placeholders for now.\n",
    "FR_FEW_SHOT_EXAMPLES = [\n",
    "    {\"review\": \"I wish there was a dark mode option in the settings.\", \"classification\": \"Feature Request\"},\n",
    "    {\"review\": \"The app crashes every time I try to upload a photo.\", \"classification\": \"Bug Report\"},\n",
    "    {\"review\": \"Great app, keep up the good work!\", \"classification\": \"Other\"},\n",
    "    {\"review\": \"Please add a way to export data to CSV.\", \"classification\": \"Feature Request\"},\n",
    "    {\"review\": \"The search filter does not work correctly for dates.\", \"classification\": \"Bug Report\"},\n",
    "    {\"review\": \"Can you make the 'save' button more prominent?\", \"classification\": \"Feature Request\"}\n",
    "]\n",
    "\n",
    "# Format these examples into a single string for the prompt\n",
    "formatted_fr_few_shot_text = \"\"\n",
    "for ex in FR_FEW_SHOT_EXAMPLES:\n",
    "    formatted_fr_few_shot_text += f\"Review: {ex['review']}\\nClassification: {ex['classification']}\\n\\n\"\n",
    "\n",
    "# Define the list of FR categories as a string for prompts\n",
    "all_fr_labels_str = \", \".join([label.capitalize() for label in VALID_FR_LABELS])\n",
    "print(\"Defined FR definitions and few-shot examples.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 4. Evaluation Function Definition (Generalized for FR) ---\n",
    "def evaluate_classification_prompt_strategy(\n",
    "    prompt_id: str,\n",
    "    data_df: pd.DataFrame,\n",
    "    client_instance: LLMClient,\n",
    "    category_name: str = \"FUNCTIONAL_REQUIREMENTS\", # Changed default category\n",
    "    **prompt_kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the classification performance using a specific prompt ID for FRs.\n",
    "\n",
    "    Args:\n",
    "        prompt_id (str): The ID of the prompt to activate from prompts.json.\n",
    "        data_df (pd.DataFrame): The DataFrame containing 'review' and 'ground_truth' columns.\n",
    "        client_instance (LLMClient): An initialized instance of the LLMClient.\n",
    "        category_name (str): The category name for the prompt in prompts.json (e.g., \"FUNCTIONAL_REQUIREMENTS\").\n",
    "        **prompt_kwargs: Additional keyword arguments to pass to the prompt's .format() method.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with 'review', 'ground_truth', 'predicted' columns,\n",
    "                      and a performance report for the given prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    Config.set_active_prompt_id(category_name, prompt_id)\n",
    "\n",
    "    print(f\"\\n--- Starting evaluation for Prompt ID: {prompt_id} ---\")\n",
    "\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, row in tqdm(data_df.iterrows(), total=len(data_df), desc=f\"Classifying with {prompt_id}\"):\n",
    "        # The classify_nfr method is generic enough to handle any text and kwargs\n",
    "        # The prompt itself determines how the classification is done.\n",
    "        response = client_instance.classify_nfr(row['review'], **prompt_kwargs)\n",
    "        pred = response.classification if response.success else \"Failed\"\n",
    "        predictions.append(pred)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Classification with {prompt_id} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "    results_df = data_df.copy()\n",
    "    results_df['predicted'] = predictions\n",
    "\n",
    "    # Standardize predicted labels for accurate comparison\n",
    "    results_df['predicted'] = results_df['predicted'].str.strip().str.lower()\n",
    "\n",
    "    # Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "    filtered_results = results_df[\n",
    "        (results_df['predicted'] != 'failed') &\n",
    "        (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "    ]\n",
    "\n",
    "    # Align the labels for classification report\n",
    "    # Ensure all labels (ground truth and predicted) are present in the report\n",
    "    unique_labels = sorted(list(set(filtered_results['ground_truth'].tolist() + filtered_results['predicted'].tolist())))\n",
    "\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Classification Report for Prompt ID: {prompt_id} ---\\n\")\n",
    "    print(f\"{report}\\n\")\n",
    "    print(f\"--- End Report for Prompt ID: {prompt_id} ---\\n\")\n",
    "\n",
    "    return results_df, report\n",
    "\n",
    "print(\"Logic setup complete. Ready to run evaluations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654cd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Evaluation for FC1 (Direct Multi-Class User Review Classification) ---\n",
    "print(\"\\n========== EVALUATING PROMPT FC1 (Direct Multi-Class) ==========\")\n",
    "results_fc1_df, report_fc1 = evaluate_classification_prompt_strategy(\n",
    "    prompt_id=\"P1\",\n",
    "    data_df=fr_data, # <--- Add .head(15) here\n",
    "    client_instance=client,\n",
    "    category_name=\"FUNCTIONAL_REQUIREMENTS\" # Specify the category\n",
    "    # No extra kwargs needed for FC1 as per its definition\n",
    ")\n",
    "print(\"First 5 predictions for FC1:\")\n",
    "print(results_fc1_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b40b9-194b-4749-a2c0-ac57d1355f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fc1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c5d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Evaluation for FC2 (Few-Shot User Review Classification) ---\n",
    "print(\"\\n========== EVALUATING PROMPT FC2 (Few-Shot) ==========\")\n",
    "results_fc2_df, report_fc2 = evaluate_classification_prompt_strategy(\n",
    "    prompt_id=\"FC2\",\n",
    "    data_df=fr_data,\n",
    "    client_instance=client,\n",
    "    category_name=\"FUNCTIONAL_REQUIREMENTS\", # Specify the category\n",
    "    few_shot_examples_text=formatted_fr_few_shot_text # Pass the prepared few-shot examples\n",
    ")\n",
    "print(\"First 5 predictions for FC2:\")\n",
    "print(results_fc2_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f380de3-1895-483b-a7bb-54913c9987e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fc2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faab12be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Evaluation for FC3 (Definition-Based User Review Classification) ---\n",
    "print(\"\\n========== EVALUATING PROMPT FC3 (Definition-Based) ==========\")\n",
    "results_fc3_df, report_fc3 = evaluate_classification_prompt_strategy(\n",
    "    prompt_id=\"FC3\",\n",
    "    data_df=fr_data,\n",
    "    client_instance=client,\n",
    "    category_name=\"FUNCTIONAL_REQUIREMENTS\" # Specify the category\n",
    "    # No extra kwargs needed for FC3, as definitions are hardcoded in the prompt itself\n",
    ")\n",
    "print(\"First 5 predictions for FC3:\")\n",
    "print(results_fc3_df.head())\n",
    "\n",
    "print(\"\\n--- All Functional Requirement evaluations completed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34006dd-ef18-495d-aa81-c37d5878b336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
