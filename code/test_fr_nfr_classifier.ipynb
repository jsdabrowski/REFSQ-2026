{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7982122-2c76-4196-a25b-c9ee2fa25db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import logging\n",
    "from src.llm_client import LLMClient\n",
    "from src.config import Config\n",
    "\n",
    "# Set up basic logging for clarity in notebook output\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "# --- CHANGE THIS LINE ---\n",
    "# Set the llm_client's logging level to INFO or WARNING to prevent excessive debug messages\n",
    "logging.getLogger('src.llm_client').setLevel(logging.INFO) # Or logging.WARNING for even less verbosity\n",
    "# --- END CHANGE ---\n",
    "\n",
    "\n",
    "# --- 1. Data Loading and Preparation for Functional Requirements ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\")\n",
    "# Load the data from BOW_test.txt\n",
    "# Assuming BOW_test.txt has 'review_text,classification' format\n",
    "fr_data = pd.read_csv(\n",
    "    \"datasets/BOW_test_sample.txt\",\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Handle potential malformed lines\n",
    ")\n",
    "\n",
    "# Standardize ground_truth labels to lowercase and strip whitespace\n",
    "fr_data['ground_truth'] = fr_data['ground_truth'].str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5cc96da0-1142-4ecb-af8d-6e6d0e89277f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "718b0aa3-0896-49d9-a117-0ed1fec47060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground_truth\n",
      "featurerequest    7\n",
      "other             7\n",
      "bugreport         6\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(fr_data['ground_truth'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016b324a-66a4-429b-b84a-200322a5adfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 functional reviews.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0  'the current fb app is not good at all for tab...    bugreport\n",
      "1  'the problem is with the way items displaypics...    bugreport\n",
      "2             'not to mention it force closes often'    bugreport\n",
      "3  'also every time i open it asks me if i want t...    bugreport\n",
      "4                     'now i cannot view any photos'    bugreport\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define valid FR categories for standardization and filtering later\n",
    "VALID_FR_LABELS = [\"featurerequest\", \"bugreport\", \"other\"]\n",
    "\n",
    "# Filter out any ground_truth labels that are not in our defined VALID_FR_LABELS\n",
    "initial_len = len(fr_data)\n",
    "fr_data = fr_data[fr_data['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown 'ground_truth' labels.\")\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3202f0-350f-45b8-ae57-d1be0304ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing LLM Client ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.llm_client:Initialized Gemini model: models/gemini-1.5-flash-latest\n",
      "INFO:src.llm_client:Initialized LLM client with https://generativelanguage.googleapis.com, model: models/gemini-1.5-flash-latest, provider: gemini\n",
      "INFO:src.llm_client:Testing Gemini API connection...\n",
      "INFO:src.llm_client:Successfully connected to Gemini API using model 'models/gemini-1.5-flash-latest'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM Client initialized and connected.\n",
      "----------------------------------------\n",
      "Defined FR definitions and few-shot examples.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 2. LLM Client Initialization ---\n",
    "print(\"--- Initializing LLM Client ---\")\n",
    "client = LLMClient()\n",
    "if not client.test_connection():\n",
    "    print(\"❌ LLM connection failed at initialization. Please check API key in config, model name, and network.\")\n",
    "    # It's good practice to exit or raise an exception here if connection is mandatory\n",
    "else:\n",
    "    print(\"✅ LLM Client initialized and connected.\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 3. Define FR Definitions & Few-Shot Examples (for FC2, FC3) ---\n",
    "# Definitions for FC3\n",
    "FR_DEFINITIONS = {\n",
    "    \"feature request\": \"A suggestion for new functionality, enhancements, or improvements to existing features.\",\n",
    "    \"bug report\": \"Describes an error, fault, or flaw in the system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways.\",\n",
    "    \"other\": \"The review does not clearly fall into Feature Request or Bug Report.\"\n",
    "}\n",
    "\n",
    "# Few-Shot Examples for FC2\n",
    "# IMPORTANT: Replace these with actual examples from your BOW_test.txt\n",
    "# if you have good, diverse examples that fit your categories.\n",
    "# These are placeholders for now.\n",
    "FR_FEW_SHOT_EXAMPLES = [\n",
    "    {\"review\": \"I wish there was a dark mode option in the settings.\", \"classification\": \"Feature Request\"},\n",
    "    {\"review\": \"The app crashes every time I try to upload a photo.\", \"classification\": \"Bug Report\"},\n",
    "    {\"review\": \"Great app, keep up the good work!\", \"classification\": \"Other\"},\n",
    "    {\"review\": \"Please add a way to export data to CSV.\", \"classification\": \"Feature Request\"},\n",
    "    {\"review\": \"The search filter does not work correctly for dates.\", \"classification\": \"Bug Report\"},\n",
    "    {\"review\": \"Can you make the 'save' button more prominent?\", \"classification\": \"Feature Request\"}\n",
    "]\n",
    "\n",
    "# Format these examples into a single string for the prompt\n",
    "formatted_fr_few_shot_text = \"\"\n",
    "for ex in FR_FEW_SHOT_EXAMPLES:\n",
    "    formatted_fr_few_shot_text += f\"Review: {ex['review']}\\nClassification: {ex['classification']}\\n\\n\"\n",
    "\n",
    "# Define the list of FR categories as a string for prompts\n",
    "all_fr_labels_str = \", \".join([label.capitalize() for label in VALID_FR_LABELS])\n",
    "print(\"Defined FR definitions and few-shot examples.\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e1a4f8-0aa7-4cdc-b6df-df062e53f9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logic setup complete. Ready to run evaluations.\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Evaluation Function Definition (Generalized for FR) ---\n",
    "def evaluate_classification_prompt_strategy(\n",
    "    prompt_id: str,\n",
    "    data_df: pd.DataFrame,\n",
    "    client_instance: LLMClient,\n",
    "    category_name: str = \"FUNCTIONAL_REQUIREMENTS\", # Changed default category\n",
    "    **prompt_kwargs\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluates the classification performance using a specific prompt ID for FRs.\n",
    "\n",
    "    Args:\n",
    "        prompt_id (str): The ID of the prompt to activate from prompts.json.\n",
    "        data_df (pd.DataFrame): The DataFrame containing 'review' and 'ground_truth' columns.\n",
    "        client_instance (LLMClient): An initialized instance of the LLMClient.\n",
    "        category_name (str): The category name for the prompt in prompts.json (e.g., \"FUNCTIONAL_REQUIREMENTS\").\n",
    "        **prompt_kwargs: Additional keyword arguments to pass to the prompt's .format() method.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with 'review', 'ground_truth', 'predicted' columns,\n",
    "                      and a performance report for the given prompt.\n",
    "    \"\"\"\n",
    "\n",
    "    Config.set_active_prompt_id(category_name, prompt_id)\n",
    "\n",
    "    print(f\"\\n--- Starting evaluation for Prompt ID: {prompt_id} ---\")\n",
    "\n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, row in tqdm(data_df.iterrows(), total=len(data_df), desc=f\"Classifying with {prompt_id}\"):\n",
    "        # The classify_nfr method is generic enough to handle any text and kwargs\n",
    "        # The prompt itself determines how the classification is done.\n",
    "        response = client_instance.classify_nfr(row['review'], **prompt_kwargs)\n",
    "        pred = response.classification if response.success else \"Failed\"\n",
    "        predictions.append(pred)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Classification with {prompt_id} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "    results_df = data_df.copy()\n",
    "    results_df['predicted'] = predictions\n",
    "\n",
    "    # Standardize predicted labels for accurate comparison\n",
    "    results_df['predicted'] = results_df['predicted'].str.strip().str.lower()\n",
    "\n",
    "    # Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "    filtered_results = results_df[\n",
    "        (results_df['predicted'] != 'failed') &\n",
    "        (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "    ]\n",
    "\n",
    "    # Align the labels for classification report\n",
    "    # Ensure all labels (ground truth and predicted) are present in the report\n",
    "    unique_labels = sorted(list(set(filtered_results['ground_truth'].tolist() + filtered_results['predicted'].tolist())))\n",
    "\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "\n",
    "    print(f\"\\n--- Classification Report for Prompt ID: {prompt_id} ---\\n\")\n",
    "    print(f\"{report}\\n\")\n",
    "    print(f\"--- End Report for Prompt ID: {prompt_id} ---\\n\")\n",
    "\n",
    "    return results_df, report\n",
    "\n",
    "print(\"Logic setup complete. Ready to run evaluations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b33216-edd2-4a17-b214-c8526e2b432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:src.config:Loaded all prompts from prompts.json\n",
      "INFO:src.config:Active prompt for 'FUNCTIONAL_REQUIREMENTS' set to 'FC1'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== EVALUATING PROMPT FC1 (Direct Multi-Class) ==========\n",
      "\n",
      "--- Starting evaluation for Prompt ID: FC1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying with FC1:  75%|█████████████████████████████████████████████               | 15/20 [00:08<00:02,  1.84it/s]ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:NFR Classification failed: Gemini request failed after 3 attempts: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "Classifying with FC1:  80%|████████████████████████████████████████████████            | 16/20 [00:08<00:01,  2.24it/s]ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:NFR Classification failed: Gemini request failed after 3 attempts: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "Classifying with FC1:  85%|███████████████████████████████████████████████████         | 17/20 [00:09<00:01,  2.64it/s]ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:NFR Classification failed: Gemini request failed after 3 attempts: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 15\n",
      "}\n",
      "]\n",
      "Classifying with FC1:  90%|██████████████████████████████████████████████████████      | 18/20 [00:09<00:00,  3.01it/s]ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:NFR Classification failed: Gemini request failed after 3 attempts: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "Classifying with FC1:  95%|█████████████████████████████████████████████████████████   | 19/20 [00:09<00:00,  3.45it/s]ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:Gemini request failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "ERROR:src.llm_client:NFR Classification failed: Gemini request failed after 3 attempts: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 15\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 14\n",
      "}\n",
      "]\n",
      "Classifying with FC1: 100%|████████████████████████████████████████████████████████████| 20/20 [00:09<00:00,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with FC1 completed in 0.16 minutes\n",
      "\n",
      "--- Classification Report for Prompt ID: FC1 ---\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "featurerequest       0.00      0.00      0.00       1.0\n",
      "     bugreport       0.00      0.00      0.00       1.0\n",
      "         other       0.00      0.00      0.00       0.0\n",
      "\n",
      "      accuracy                           0.00       2.0\n",
      "     macro avg       0.00      0.00      0.00       2.0\n",
      "  weighted avg       0.00      0.00      0.00       2.0\n",
      "\n",
      "\n",
      "--- End Report for Prompt ID: FC1 ---\n",
      "\n",
      "First 5 predictions for FC1:\n",
      "                                              review ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...    bugreport   \n",
      "1  'the problem is with the way items displaypics...    bugreport   \n",
      "2             'not to mention it force closes often'    bugreport   \n",
      "3  'also every time i open it asks me if i want t...    bugreport   \n",
      "4                     'now i cannot view any photos'    bugreport   \n",
      "\n",
      "         predicted  \n",
      "0        usability  \n",
      "1    look and feel  \n",
      "2  fault tolerance  \n",
      "3        usability  \n",
      "4            other  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Evaluation for FC1 (Direct Multi-Class User Review Classification) ---\n",
    "print(\"\\n========== EVALUATING PROMPT FC1 (Direct Multi-Class) ==========\")\n",
    "results_fc1_df, report_fc1 = evaluate_classification_prompt_strategy(\n",
    "    prompt_id=\"FC1\",\n",
    "    data_df=fr_data, # <--- Add .head(15) here\n",
    "    client_instance=client,\n",
    "    category_name=\"FUNCTIONAL_REQUIREMENTS\" # Specify the category\n",
    "    # No extra kwargs needed for FC1 as per its definition\n",
    ")\n",
    "print(\"First 5 predictions for FC1:\")\n",
    "print(results_fc1_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2022c60-f2f1-48e3-8003-892ce8b5defb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'the current fb app is not good at all for tab...</td>\n",
       "      <td>bugreport</td>\n",
       "      <td>usability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'the problem is with the way items displaypics...</td>\n",
       "      <td>bugreport</td>\n",
       "      <td>look and feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'not to mention it force closes often'</td>\n",
       "      <td>bugreport</td>\n",
       "      <td>fault tolerance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'also every time i open it asks me if i want t...</td>\n",
       "      <td>bugreport</td>\n",
       "      <td>usability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'now i cannot view any photos'</td>\n",
       "      <td>bugreport</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'loading takes time even on 3g con it totaly s...</td>\n",
       "      <td>bugreport</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'it should have multi tabs '</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>usability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'for me to be able to post a status i need to ...</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>operability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'i use my phone almost  exclusivly to log into...</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>usability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'cant turn off location tracking'</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'my biggest pet peeve is that i cant like'</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'would be better if it could be moved to sd card'</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>portability</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'games cannot link with facebook '</td>\n",
       "      <td>featurerequest</td>\n",
       "      <td>security</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'But why does it take up 27 MB of RAM on my Ga...</td>\n",
       "      <td>other</td>\n",
       "      <td>performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'Arabic letters are rather small'</td>\n",
       "      <td>other</td>\n",
       "      <td>look and feel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'It also takes a lot of memory'</td>\n",
       "      <td>other</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'this is the best keyboard ever but it eats qu...</td>\n",
       "      <td>other</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'I often find myself accidentally deleting words'</td>\n",
       "      <td>other</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'SmartSpace is terrible unadjustable and too i...</td>\n",
       "      <td>other</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'SwiftKey doesnt like the Moto LapDock at all'</td>\n",
       "      <td>other</td>\n",
       "      <td>failed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review    ground_truth  \\\n",
       "0   'the current fb app is not good at all for tab...       bugreport   \n",
       "1   'the problem is with the way items displaypics...       bugreport   \n",
       "2              'not to mention it force closes often'       bugreport   \n",
       "3   'also every time i open it asks me if i want t...       bugreport   \n",
       "4                      'now i cannot view any photos'       bugreport   \n",
       "5   'loading takes time even on 3g con it totaly s...       bugreport   \n",
       "6                        'it should have multi tabs '  featurerequest   \n",
       "7   'for me to be able to post a status i need to ...  featurerequest   \n",
       "8   'i use my phone almost  exclusivly to log into...  featurerequest   \n",
       "9                   'cant turn off location tracking'  featurerequest   \n",
       "10         'my biggest pet peeve is that i cant like'  featurerequest   \n",
       "11  'would be better if it could be moved to sd card'  featurerequest   \n",
       "12                 'games cannot link with facebook '  featurerequest   \n",
       "13  'But why does it take up 27 MB of RAM on my Ga...           other   \n",
       "14                  'Arabic letters are rather small'           other   \n",
       "15                    'It also takes a lot of memory'           other   \n",
       "16  'this is the best keyboard ever but it eats qu...           other   \n",
       "17  'I often find myself accidentally deleting words'           other   \n",
       "18  'SmartSpace is terrible unadjustable and too i...           other   \n",
       "19     'SwiftKey doesnt like the Moto LapDock at all'           other   \n",
       "\n",
       "          predicted  \n",
       "0         usability  \n",
       "1     look and feel  \n",
       "2   fault tolerance  \n",
       "3         usability  \n",
       "4             other  \n",
       "5       performance  \n",
       "6         usability  \n",
       "7       operability  \n",
       "8         usability  \n",
       "9          security  \n",
       "10            other  \n",
       "11      portability  \n",
       "12         security  \n",
       "13      performance  \n",
       "14    look and feel  \n",
       "15           failed  \n",
       "16           failed  \n",
       "17           failed  \n",
       "18           failed  \n",
       "19           failed  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_fc1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6483bce2-8aa0-4a4d-b9c8-9477af4aaf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Testing Custom Inputs with Llama 2 (llama2) and Chain of Thought ==========\n",
      "\n",
      "--- Review 1: I'd love to see a dark mode option in the settings for better night viewing. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Step 1: Analyze the user's statement for intent\n",
      "The user explicitly states their desire for a new functionality, specifically a dark mode option in the settings for better night viewing. This indicates that they are making a request for something new or improved, which falls under the category of Feature Request.\n",
      "\n",
      "Step 2: Determine if it explicitly asks for something new or improved\n",
      "Yes, the user is asking for a new feature related to the dark mode option in the settings. They want the option to be available for better night viewing, indicating that they would like to see this functionality added or improved.\n",
      "\n",
      "Step 3: Determine if it describes a fault, error, or unintended behavior (Bug Report)\n",
      "No, the user's statement does not describe any fault, error, or unintended behavior. It is purely a request for new functionality, so it falls under the category of Feature Request.\n",
      "\n",
      "Step 4: If neither, classify as Other\n",
      "Since the user's statement does not describe a problem or fault, and it is not a general comment, it can be classified as a Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 2: Could you add a feature to export all my saved articles to PDF? ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Explanation:\n",
      "The user's review is requesting a new feature to export all saved articles to PDF. This falls under the category of a Feature Request, as it is asking for an addition to the existing functionality of the software. The user is not reporting any bugs or issues with the current implementation, but rather suggesting a new capability that they would find useful.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 3: Please implement a search filter to sort results by publication date. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Explanation: The user review is requesting for a new functionality, specifically a search filter to sort results by publication date. The user is not reporting any bugs or issues with the existing functionality, but rather asking for an improvement to make the product more useful to them. Therefore, it falls under the category of Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 4: It would be very convenient if I could set custom notification sounds for different contacts. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Explanation: The user is requesting a new functionality that would allow them to set custom notification sounds for different contacts. This is an explicit request for something new or improved, which falls under the category of a Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 5: A 'mark all as read' button for messages would greatly improve efficiency. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Explanation: The user's review is requesting for a new functionality, specifically a \"mark all as read\" button for messages. This is not a bug report or any other type of issue, but rather a feature request that would improve the efficiency of the application.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Custom Test Complete ---\n",
      "\n",
      "Final Parsed Classifications: ['feature request', 'feature request', 'feature request', 'feature request', 'feature request']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "import os # Included for completeness, though not strictly used for LLM interaction in this independent cell\n",
    "\n",
    "# --- Basic Logging Setup ---\n",
    "# This ensures logging messages from this cell are formatted.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- LLM Configuration (Hardcoded for this independent test) ---\n",
    "# IMPORTANT: This configuration is LOCAL to this cell and does NOT affect src/config.py\n",
    "LLAMA2_MODEL_NAME = \"llama2\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\" # Default Ollama API endpoint\n",
    "\n",
    "# --- Custom Input Data (Feature Requests) ---\n",
    "custom_feature_request_data = pd.DataFrame({\n",
    "    'review': [\n",
    "        \"I'd love to see a dark mode option in the settings for better night viewing.\",\n",
    "        \"Could you add a feature to export all my saved articles to PDF?\",\n",
    "        \"Please implement a search filter to sort results by publication date.\",\n",
    "        \"It would be very convenient if I could set custom notification sounds for different contacts.\",\n",
    "        \"A 'mark all as read' button for messages would greatly improve efficiency.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# --- Custom Chain-of-Thought Prompt with STRICT Output Format ---\n",
    "cot_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis. Your task is to classify user reviews.\n",
    "The allowed functional requirement categories are: Feature Request, Bug Report, Other.\n",
    "Do NOT use non-functional requirement categories like Usability, Performance, Security, etc.\n",
    "\n",
    "For the following user review, first, explain your reasoning step-by-step why you are classifying it as a 'Feature Request', 'Bug Report', or 'Other'.\n",
    "Finally, on a new line, provide the classification in the EXACT format: \"CLASSIFICATION: [Category Name]\". For example, \"CLASSIFICATION: Feature Request\".\n",
    "\n",
    "User Review: {review_text}\n",
    "\n",
    "Reasoning:\n",
    "1. Analyze the user's statement for intent (new functionality, problem, or general comment).\n",
    "2. Determine if it explicitly asks for something new or improved (Feature Request).\n",
    "3. Determine if it describes a fault, error, or unintended behavior (Bug Report).\n",
    "4. If neither, classify as Other.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n========== Testing Custom Inputs with Llama 2 ({LLAMA2_MODEL_NAME}) and Chain of Thought ==========\")\n",
    "\n",
    "# --- Llama 2 (Ollama) Interaction Logic ---\n",
    "def classify_with_llama2_cot(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a request to the local Ollama Llama 2 model with the CoT prompt\n",
    "    and returns the raw response.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Format the CoT prompt with the current review text\n",
    "    formatted_prompt = cot_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLAMA2_MODEL_NAME,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False # We want the full response at once\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\"), \"classification\": None}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\", \"classification\": None}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\", \"classification\": None}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\", \"classification\": None}\n",
    "\n",
    "# --- Run Classification for Custom Inputs ---\n",
    "cot_predictions = []\n",
    "for i, row in custom_feature_request_data.iterrows():\n",
    "    print(f\"\\n--- Review {i+1}: {row['review']} ---\")\n",
    "    response_data = classify_with_llama2_cot(row['review'])\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        # Print the raw model output to see the reasoning\n",
    "        print(\"\\nLLM Raw Output (with Reasoning):\\n\", response_data[\"raw_response\"])\n",
    "        \n",
    "        # --- SIMPLIFIED REGEX FOR PARSING (due to strict prompt instruction) ---\n",
    "        # Now we expect \"CLASSIFICATION: [Category Name]\"\n",
    "        match = re.search(\n",
    "            r\"CLASSIFICATION:\\s*(Feature Request|Bug Report|Other)\",\n",
    "            response_data[\"raw_response\"],\n",
    "            re.IGNORECASE | re.DOTALL # DOTALL is useful if CLASSIFICATION: is on a different line\n",
    "        )\n",
    "        # --- END SIMPLIFIED REGEX ---\n",
    "\n",
    "        pred_class = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        cot_predictions.append(pred_class)\n",
    "        print(f\"Parsed Classification: {pred_class}\")\n",
    "    else:\n",
    "        print(f\"LLM Call Failed: {response_data['raw_response']}\")\n",
    "        cot_predictions.append(\"Failed\")\n",
    "\n",
    "print(\"\\n--- Custom Test Complete ---\")\n",
    "\n",
    "# You can manually inspect cot_predictions list here to see parsed results\n",
    "print(\"\\nFinal Parsed Classifications:\", cot_predictions)\n",
    "\n",
    "# Optional: To see how it would perform in a report, assuming ground truth is correct\n",
    "# from sklearn.metrics import classification_report\n",
    "#\n",
    "# # Filter out failed predictions for reporting\n",
    "# results_df_temp = custom_feature_request_data.copy()\n",
    "# results_df_temp['predicted'] = cot_predictions\n",
    "#\n",
    "# # Assign ground truth for this specific test case (all are 'feature request')\n",
    "# results_df_temp['ground_truth'] = ['feature request'] * len(custom_feature_request_data)\n",
    "#\n",
    "# filtered_results = results_df_temp[results_df_temp['predicted'] != 'failed']\n",
    "# print(\"\\n--- Classification Report for Custom Test ---\")\n",
    "# print(classification_report(\n",
    "#     filtered_results['ground_truth'],\n",
    "#     filtered_results['predicted'],\n",
    "#     labels=[\"feature request\", \"bug report\", \"other\"],\n",
    "#     zero_division=0\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6b67972-1193-44f9-ab38-3e28000c51d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Testing Custom Inputs with Llama 2 (llama2) and Chain of Thought - Bug Reports ==========\n",
      "\n",
      "--- Review 1: The app crashes every time I try to open a PDF document. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Reasoning:\n",
      "Step 1: The user's statement is focused on a problem they are experiencing with the app, specifically that it crashes every time they try to open a PDF document. This suggests that the user is asking for improved functionality or a new feature to address this issue.\n",
      "\n",
      "Step 2: The user's statement does not explicitly ask for something new or improved. It simply describes a problem they are experiencing with the app, which indicates a Feature Request classification.\n",
      "\n",
      "Step 3: The user's statement does not describe any fault, error, or unintended behavior that would warrant a Bug Report classification.\n",
      "\n",
      "Step 4: Since the user's statement is focused on a problem they are experiencing with the app and does not request any new functionality or improved features, it is classified as a Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 2: When I click 'save', the changes are not actually reflected, and the old data remains. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "The user's review indicates that they are experiencing a problem with the software not reflecting changes made after clicking \"save\". This is a common feature request in software applications, where users expect their changes to be saved and reflected upon next opening or accessing the application. Therefore, this review falls under the category of Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 3: The search bar freezes and becomes unresponsive after typing more than 10 characters. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Explanation: The user's review mentions that the search bar becomes unresponsive after typing more than 10 characters, which suggests a limitation in the current functionality of the software. The user is requesting for an improvement in the search bar's ability to handle longer inputs, which falls under the category of a Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 4: Users are unable to log in from Android devices; it constantly shows an 'invalid credentials' error. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "Reasoning:\n",
      "1. Intent: The user's statement is focused on a problem they are experiencing with logging in from Android devices, which suggests that they want to use the application on those devices. This implies that they see the application as having potential for improvement or expansion.\n",
      "2. Explicit Ask: The user does not explicitly ask for something new or improved, but the context of the statement indicates that they are interested in using the application on Android devices, which could be seen as a Feature Request.\n",
      "3. No description of fault, error, or unintended behavior that would qualify it as a Bug Report.\n",
      "\n",
      "Therefore, based on the reasoning above, the user review is classified as a Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Review 5: The push notifications are not delivering consistently, sometimes they arrive hours late. ---\n",
      "\n",
      "LLM Raw Output (with Reasoning):\n",
      " CLASSIFICATION: Feature Request\n",
      "\n",
      "The user's review states that the push notifications are not delivering consistently and sometimes arrive hours late. This implies that the user would like to see improved reliability in the delivery of push notifications, which falls under the category of a Feature Request. The user is not asking for anything new or improved, nor are they reporting a fault, error, or unintended behavior, so it does not qualify as a Bug Report. Therefore, the classification is a Feature Request.\n",
      "Parsed Classification: feature request\n",
      "\n",
      "--- Custom Bug Report Test Complete ---\n",
      "\n",
      "Final Parsed Classifications (Bug Reports): ['feature request', 'feature request', 'feature request', 'feature request', 'feature request']\n",
      "\n",
      "--- Classification Report for Custom Bug Report Test ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.00      0.00      0.00       0.0\n",
      "     bug report       0.00      0.00      0.00       5.0\n",
      "          other       0.00      0.00      0.00       0.0\n",
      "\n",
      "       accuracy                           0.00       5.0\n",
      "      macro avg       0.00      0.00      0.00       5.0\n",
      "   weighted avg       0.00      0.00      0.00       5.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Assuming classify_with_llama2_cot and cot_prompt_text are defined from the previous cell\n",
    "\n",
    "# --- Custom Input Data (Bug Reports) ---\n",
    "custom_bug_report_data = pd.DataFrame({\n",
    "    'review': [\n",
    "        \"The app crashes every time I try to open a PDF document.\",\n",
    "        \"When I click 'save', the changes are not actually reflected, and the old data remains.\",\n",
    "        \"The search bar freezes and becomes unresponsive after typing more than 10 characters.\",\n",
    "        \"Users are unable to log in from Android devices; it constantly shows an 'invalid credentials' error.\",\n",
    "        \"The push notifications are not delivering consistently, sometimes they arrive hours late.\"\n",
    "    ],\n",
    "    'ground_truth': [ # Assign ground truth for this specific test\n",
    "        \"bug report\",\n",
    "        \"bug report\",\n",
    "        \"bug report\",\n",
    "        \"bug report\",\n",
    "        \"bug report\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(f\"\\n========== Testing Custom Inputs with Llama 2 (llama2) and Chain of Thought - Bug Reports ==========\")\n",
    "\n",
    "# --- Run Classification for Custom Bug Report Inputs ---\n",
    "bug_report_predictions = []\n",
    "for i, row in custom_bug_report_data.iterrows():\n",
    "    print(f\"\\n--- Review {i+1}: {row['review']} ---\")\n",
    "    response_data = classify_with_llama2_cot(row['review']) # Reusing the function from previous cell\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        print(\"\\nLLM Raw Output (with Reasoning):\\n\", response_data[\"raw_response\"])\n",
    "        \n",
    "        # Reusing the same parsing logic, which should now be robust\n",
    "        match = re.search(\n",
    "            r\"CLASSIFICATION:\\s*(Feature Request|Bug Report|Other)\",\n",
    "            response_data[\"raw_response\"],\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        pred_class = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        bug_report_predictions.append(pred_class)\n",
    "        print(f\"Parsed Classification: {pred_class}\")\n",
    "    else:\n",
    "        print(f\"LLM Call Failed: {response_data['raw_response']}\")\n",
    "        bug_report_predictions.append(\"Failed\")\n",
    "\n",
    "print(\"\\n--- Custom Bug Report Test Complete ---\")\n",
    "\n",
    "# You can manually inspect bug_report_predictions list here\n",
    "print(\"\\nFinal Parsed Classifications (Bug Reports):\", bug_report_predictions)\n",
    "\n",
    "# Optional: Classification Report for this specific test set\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "results_df_bug_report = custom_bug_report_data.copy()\n",
    "results_df_bug_report['predicted'] = bug_report_predictions\n",
    "\n",
    "filtered_results_bug_report = results_df_bug_report[results_df_bug_report['predicted'] != 'failed']\n",
    "print(\"\\n--- Classification Report for Custom Bug Report Test ---\")\n",
    "print(classification_report(\n",
    "    filtered_results_bug_report['ground_truth'],\n",
    "    filtered_results_bug_report['predicted'],\n",
    "    labels=[\"feature request\", \"bug report\", \"other\"], # Ensure all possible labels are listed\n",
    "    zero_division=0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "445fe271-0cde-4048-b412-d758e9b9ebc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\n",
      "Loaded and sampled 20 functional reviews for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review     ground_truth\n",
      "0  'the current fb app is not good at all for tab...       bug report\n",
      "1  'I often find myself accidentally deleting words'            other\n",
      "2                    'It also takes a lot of memory'            other\n",
      "3  'the problem is with the way items displaypics...       bug report\n",
      "4  'i use my phone almost  exclusivly to log into...  feature request\n",
      "----------------------------------------\n",
      "\n",
      "========== Starting Classification Evaluation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2: 100%|█████████████████████████████████████████████████| 20/20 [01:03<00:00,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification completed in 1.06 minutes\n",
      "\n",
      "--- Sample of Predictions ---\n",
      "                                              review     ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...       bug report   \n",
      "1  'I often find myself accidentally deleting words'            other   \n",
      "2                    'It also takes a lot of memory'            other   \n",
      "3  'the problem is with the way items displaypics...       bug report   \n",
      "4  'i use my phone almost  exclusivly to log into...  feature request   \n",
      "\n",
      "    predicted  \n",
      "0  bug report  \n",
      "1  bug report  \n",
      "2  bug report  \n",
      "3  bug report  \n",
      "4  bug report  \n",
      "\n",
      "--- Classification Report for llama2 ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.60      0.43      0.50         7\n",
      "     bug report       0.33      0.83      0.48         6\n",
      "          other       0.00      0.00      0.00         7\n",
      "\n",
      "       accuracy                           0.40        20\n",
      "      macro avg       0.31      0.42      0.33        20\n",
      "   weighted avg       0.31      0.40      0.32        20\n",
      "\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "# --- 1. Logging Setup (for this self-contained cell) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration (Hardcoded for this independent test) ---\n",
    "LLAMA2_MODEL_NAME = \"llama2\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- 3. Data Loading and Preparation ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\")\n",
    "excel_path = \"datasets/BOW_test_sample.txt\" # Changed to BOW_test_sample.txt\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    excel_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- NEW: Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# Create a mapping for your specific input labels\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other' # Ensure 'other' also goes through for consistency\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels (should be fewer now)\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# Sample data to keep it less than or equal to 20 for quick testing\n",
    "# min(20, len(fr_data)) ensures we don't try to sample more rows than available\n",
    "fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded and sampled {len(fr_data)} functional reviews for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function (adapted for this cell) ---\n",
    "def classify_with_llama2(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama Llama 2 model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Format the prompt with the current review text\n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLAMA2_MODEL_NAME,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False, # Get full response at once\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep temperature low for consistent classification\n",
    "            \"num_predict\": 100 # Limit output length to prevent rambling\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120) # Added timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: {review_text[:50]}...\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation Loop ---\n",
    "print(\"\\n========== Starting Classification Evaluation ==========\")\n",
    "\n",
    "predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {LLAMA2_MODEL_NAME}\"):\n",
    "    response_data = classify_with_llama2(row['review'])\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        predicted_raw = response_data[\"raw_response\"].strip()\n",
    "        \n",
    "        # Regex to capture the exact category name after \"Classification:\"\n",
    "        match = re.search(\n",
    "            r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\", # Non-capturing group for \"CLASSIFICATION: \" is optional\n",
    "            predicted_raw,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        \n",
    "        # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "        # print(f\"Raw LLM Output for '{row['review'][:50]}...': '{predicted_raw}' -> Parsed: '{pred}'\")\n",
    "\n",
    "    else:\n",
    "        pred = \"Failed\"\n",
    "        logger.warning(f\"Classification failed for review: {row['review'][:50]}... Error: {response_data['raw_response']}\")\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Classification completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "# --- 7. Prepare Results and Generate Classification Report ---\n",
    "results_df = fr_data.copy()\n",
    "results_df['predicted'] = predictions\n",
    "\n",
    "# Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "filtered_results = results_df[\n",
    "    (results_df['predicted'] != 'failed') &\n",
    "    (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sample of Predictions ---\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(f\"\\n--- Classification Report for {LLAMA2_MODEL_NAME} ---\")\n",
    "if not filtered_results.empty:\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No valid predictions to generate a classification report.\")\n",
    "    \n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0173db7c-8e40-4d9e-ab18-842628b9f925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\n",
      "Loaded and sampled 20 functional reviews for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review     ground_truth\n",
      "0  'the current fb app is not good at all for tab...       bug report\n",
      "1  'I often find myself accidentally deleting words'            other\n",
      "2                    'It also takes a lot of memory'            other\n",
      "3  'the problem is with the way items displaypics...       bug report\n",
      "4  'i use my phone almost  exclusivly to log into...  feature request\n",
      "----------------------------------------\n",
      "\n",
      "========== Starting Classification Evaluation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral: 100%|████████████████████████████████████████████████| 20/20 [00:56<00:00,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification completed in 0.95 minutes\n",
      "\n",
      "--- Sample of Predictions ---\n",
      "                                              review     ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...       bug report   \n",
      "1  'I often find myself accidentally deleting words'            other   \n",
      "2                    'It also takes a lot of memory'            other   \n",
      "3  'the problem is with the way items displaypics...       bug report   \n",
      "4  'i use my phone almost  exclusivly to log into...  feature request   \n",
      "\n",
      "         predicted  \n",
      "0            other  \n",
      "1            other  \n",
      "2            other  \n",
      "3            other  \n",
      "4  feature request  \n",
      "\n",
      "--- Classification Report for mistral ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       1.00      0.71      0.83         7\n",
      "     bug report       0.60      0.50      0.55         6\n",
      "          other       0.60      0.86      0.71         7\n",
      "\n",
      "       accuracy                           0.70        20\n",
      "      macro avg       0.73      0.69      0.69        20\n",
      "   weighted avg       0.74      0.70      0.70        20\n",
      "\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "# --- 1. Logging Setup (for this self-contained cell) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration (Hardcoded for this independent test) ---\n",
    "LLAMA2_MODEL_NAME = \"mistral\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- 3. Data Loading and Preparation ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\")\n",
    "excel_path = \"datasets/BOW_test_sample.txt\" # Changed to BOW_test_sample.txt\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    excel_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- NEW: Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# Create a mapping for your specific input labels\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other' # Ensure 'other' also goes through for consistency\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels (should be fewer now)\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# Sample data to keep it less than or equal to 20 for quick testing\n",
    "# min(20, len(fr_data)) ensures we don't try to sample more rows than available\n",
    "fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded and sampled {len(fr_data)} functional reviews for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function (adapted for this cell) ---\n",
    "def classify_with_llama2(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama Llama 2 model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Format the prompt with the current review text\n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLAMA2_MODEL_NAME,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False, # Get full response at once\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep temperature low for consistent classification\n",
    "            \"num_predict\": 100 # Limit output length to prevent rambling\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120) # Added timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: {review_text[:50]}...\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation Loop ---\n",
    "print(\"\\n========== Starting Classification Evaluation ==========\")\n",
    "\n",
    "predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {LLAMA2_MODEL_NAME}\"):\n",
    "    response_data = classify_with_llama2(row['review'])\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        predicted_raw = response_data[\"raw_response\"].strip()\n",
    "        \n",
    "        # Regex to capture the exact category name after \"Classification:\"\n",
    "        match = re.search(\n",
    "            r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\", # Non-capturing group for \"CLASSIFICATION: \" is optional\n",
    "            predicted_raw,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        \n",
    "        # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "        # print(f\"Raw LLM Output for '{row['review'][:50]}...': '{predicted_raw}' -> Parsed: '{pred}'\")\n",
    "\n",
    "    else:\n",
    "        pred = \"Failed\"\n",
    "        logger.warning(f\"Classification failed for review: {row['review'][:50]}... Error: {response_data['raw_response']}\")\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Classification completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "# --- 7. Prepare Results and Generate Classification Report ---\n",
    "results_df = fr_data.copy()\n",
    "results_df['predicted'] = predictions\n",
    "\n",
    "# Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "filtered_results = results_df[\n",
    "    (results_df['predicted'] != 'failed') &\n",
    "    (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sample of Predictions ---\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(f\"\\n--- Classification Report for {LLAMA2_MODEL_NAME} ---\")\n",
    "if not filtered_results.empty:\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No valid predictions to generate a classification report.\")\n",
    "    \n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f79811-01d8-4824-93a6-4ad186641e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\n",
      "Loaded and sampled 20 functional reviews for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review     ground_truth\n",
      "0  'the current fb app is not good at all for tab...       bug report\n",
      "1  'I often find myself accidentally deleting words'            other\n",
      "2                    'It also takes a lot of memory'            other\n",
      "3  'the problem is with the way items displaypics...       bug report\n",
      "4  'i use my phone almost  exclusivly to log into...  feature request\n",
      "----------------------------------------\n",
      "\n",
      "========== Starting Classification Evaluation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b: 100%|██████████████████████████████████████████████| 20/20 [01:06<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification completed in 1.10 minutes\n",
      "\n",
      "--- Sample of Predictions ---\n",
      "                                              review     ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...       bug report   \n",
      "1  'I often find myself accidentally deleting words'            other   \n",
      "2                    'It also takes a lot of memory'            other   \n",
      "3  'the problem is with the way items displaypics...       bug report   \n",
      "4  'i use my phone almost  exclusivly to log into...  feature request   \n",
      "\n",
      "    predicted  \n",
      "0  bug report  \n",
      "1  bug report  \n",
      "2  bug report  \n",
      "3  bug report  \n",
      "4  bug report  \n",
      "\n",
      "--- Classification Report for llama3:8b ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       1.00      0.14      0.25         7\n",
      "     bug report       0.32      1.00      0.48         6\n",
      "          other       0.00      0.00      0.00         7\n",
      "\n",
      "       accuracy                           0.35        20\n",
      "      macro avg       0.44      0.38      0.24        20\n",
      "   weighted avg       0.44      0.35      0.23        20\n",
      "\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "# --- 1. Logging Setup (for this self-contained cell) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration (Hardcoded for this independent test) ---\n",
    "LLAMA2_MODEL_NAME = \"llama3:8b\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- 3. Data Loading and Preparation ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\")\n",
    "excel_path = \"datasets/BOW_test_sample.txt\" # Changed to BOW_test_sample.txt\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    excel_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- NEW: Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# Create a mapping for your specific input labels\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other' # Ensure 'other' also goes through for consistency\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels (should be fewer now)\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# Sample data to keep it less than or equal to 20 for quick testing\n",
    "# min(20, len(fr_data)) ensures we don't try to sample more rows than available\n",
    "fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded and sampled {len(fr_data)} functional reviews for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function (adapted for this cell) ---\n",
    "def classify_with_llama2(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama Llama 2 model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Format the prompt with the current review text\n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLAMA2_MODEL_NAME,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False, # Get full response at once\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep temperature low for consistent classification\n",
    "            \"num_predict\": 100 # Limit output length to prevent rambling\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120) # Added timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: {review_text[:50]}...\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation Loop ---\n",
    "print(\"\\n========== Starting Classification Evaluation ==========\")\n",
    "\n",
    "predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {LLAMA2_MODEL_NAME}\"):\n",
    "    response_data = classify_with_llama2(row['review'])\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        predicted_raw = response_data[\"raw_response\"].strip()\n",
    "        \n",
    "        # Regex to capture the exact category name after \"Classification:\"\n",
    "        match = re.search(\n",
    "            r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\", # Non-capturing group for \"CLASSIFICATION: \" is optional\n",
    "            predicted_raw,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        \n",
    "        # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "        # print(f\"Raw LLM Output for '{row['review'][:50]}...': '{predicted_raw}' -> Parsed: '{pred}'\")\n",
    "\n",
    "    else:\n",
    "        pred = \"Failed\"\n",
    "        logger.warning(f\"Classification failed for review: {row['review'][:50]}... Error: {response_data['raw_response']}\")\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Classification completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "# --- 7. Prepare Results and Generate Classification Report ---\n",
    "results_df = fr_data.copy()\n",
    "results_df['predicted'] = predictions\n",
    "\n",
    "# Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "filtered_results = results_df[\n",
    "    (results_df['predicted'] != 'failed') &\n",
    "    (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sample of Predictions ---\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(f\"\\n--- Classification Report for {LLAMA2_MODEL_NAME} ---\")\n",
    "if not filtered_results.empty:\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No valid predictions to generate a classification report.\")\n",
    "    \n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc5fba3-ffdc-4d0f-9371-dcbf8acf1ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\n",
      "Loaded and sampled 20 functional reviews for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review     ground_truth\n",
      "0  'the current fb app is not good at all for tab...       bug report\n",
      "1  'I often find myself accidentally deleting words'            other\n",
      "2                    'It also takes a lot of memory'            other\n",
      "3  'the problem is with the way items displaypics...       bug report\n",
      "4  'i use my phone almost  exclusivly to log into...  feature request\n",
      "----------------------------------------\n",
      "\n",
      "========== Starting Classification Evaluation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b: 100%|███████████████████████████████████████████████| 20/20 [01:18<00:00,  3.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification completed in 1.32 minutes\n",
      "\n",
      "--- Sample of Predictions ---\n",
      "                                              review     ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...       bug report   \n",
      "1  'I often find myself accidentally deleting words'            other   \n",
      "2                    'It also takes a lot of memory'            other   \n",
      "3  'the problem is with the way items displaypics...       bug report   \n",
      "4  'i use my phone almost  exclusivly to log into...  feature request   \n",
      "\n",
      "         predicted  \n",
      "0  feature request  \n",
      "1       bug report  \n",
      "2       bug report  \n",
      "3  feature request  \n",
      "4  feature request  \n",
      "\n",
      "--- Classification Report for gemma:7b ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.42      0.71      0.53         7\n",
      "     bug report       0.38      0.50      0.43         6\n",
      "          other       0.00      0.00      0.00         7\n",
      "\n",
      "       accuracy                           0.40        20\n",
      "      macro avg       0.26      0.40      0.32        20\n",
      "   weighted avg       0.26      0.40      0.31        20\n",
      "\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "# --- 1. Logging Setup (for this self-contained cell) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration (Hardcoded for this independent test) ---\n",
    "LLAMA2_MODEL_NAME = \"gemma:7b\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- 3. Data Loading and Preparation ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\")\n",
    "excel_path = \"datasets/BOW_test_sample.txt\" # Changed to BOW_test_sample.txt\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    excel_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- NEW: Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# Create a mapping for your specific input labels\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other' # Ensure 'other' also goes through for consistency\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels (should be fewer now)\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# Sample data to keep it less than or equal to 20 for quick testing\n",
    "# min(20, len(fr_data)) ensures we don't try to sample more rows than available\n",
    "fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded and sampled {len(fr_data)} functional reviews for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function (adapted for this cell) ---\n",
    "def classify_with_llama2(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama Llama 2 model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Format the prompt with the current review text\n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLAMA2_MODEL_NAME,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False, # Get full response at once\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep temperature low for consistent classification\n",
    "            \"num_predict\": 100 # Limit output length to prevent rambling\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120) # Added timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: {review_text[:50]}...\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation Loop ---\n",
    "print(\"\\n========== Starting Classification Evaluation ==========\")\n",
    "\n",
    "predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {LLAMA2_MODEL_NAME}\"):\n",
    "    response_data = classify_with_llama2(row['review'])\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        predicted_raw = response_data[\"raw_response\"].strip()\n",
    "        \n",
    "        # Regex to capture the exact category name after \"Classification:\"\n",
    "        match = re.search(\n",
    "            r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\", # Non-capturing group for \"CLASSIFICATION: \" is optional\n",
    "            predicted_raw,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        \n",
    "        # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "        # print(f\"Raw LLM Output for '{row['review'][:50]}...': '{predicted_raw}' -> Parsed: '{pred}'\")\n",
    "\n",
    "    else:\n",
    "        pred = \"Failed\"\n",
    "        logger.warning(f\"Classification failed for review: {row['review'][:50]}... Error: {response_data['raw_response']}\")\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Classification completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "# --- 7. Prepare Results and Generate Classification Report ---\n",
    "results_df = fr_data.copy()\n",
    "results_df['predicted'] = predictions\n",
    "\n",
    "# Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "filtered_results = results_df[\n",
    "    (results_df['predicted'] != 'failed') &\n",
    "    (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sample of Predictions ---\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(f\"\\n--- Classification Report for {LLAMA2_MODEL_NAME} ---\")\n",
    "if not filtered_results.empty:\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No valid predictions to generate a classification report.\")\n",
    "    \n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd826b9-6a00-4bbc-9241-f4bb457e6208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\n",
      "Loaded and sampled 20 functional reviews for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review     ground_truth\n",
      "0  'the current fb app is not good at all for tab...       bug report\n",
      "1  'I often find myself accidentally deleting words'            other\n",
      "2                    'It also takes a lot of memory'            other\n",
      "3  'the problem is with the way items displaypics...       bug report\n",
      "4  'i use my phone almost  exclusivly to log into...  feature request\n",
      "----------------------------------------\n",
      "\n",
      "========== Starting Classification Evaluation ==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with phi3:mini: 100%|██████████████████████████████████████████████| 20/20 [00:49<00:00,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification completed in 0.82 minutes\n",
      "\n",
      "--- Sample of Predictions ---\n",
      "                                              review     ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...       bug report   \n",
      "1  'I often find myself accidentally deleting words'            other   \n",
      "2                    'It also takes a lot of memory'            other   \n",
      "3  'the problem is with the way items displaypics...       bug report   \n",
      "4  'i use my phone almost  exclusivly to log into...  feature request   \n",
      "\n",
      "    predicted  \n",
      "0       other  \n",
      "1  bug report  \n",
      "2  bug report  \n",
      "3  bug report  \n",
      "4  bug report  \n",
      "\n",
      "--- Classification Report for phi3:mini ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       1.00      0.43      0.60         7\n",
      "     bug report       0.33      0.83      0.48         6\n",
      "          other       0.50      0.14      0.22         7\n",
      "\n",
      "       accuracy                           0.45        20\n",
      "      macro avg       0.61      0.47      0.43        20\n",
      "   weighted avg       0.62      0.45      0.43        20\n",
      "\n",
      "\n",
      "--- Evaluation Complete ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from tqdm import tqdm # Import tqdm\n",
    "\n",
    "# --- 1. Logging Setup (for this self-contained cell) ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration (Hardcoded for this independent test) ---\n",
    "LLAMA2_MODEL_NAME = \"phi3:mini\"\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# --- 3. Data Loading and Preparation ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt ---\")\n",
    "excel_path = \"datasets/BOW_test_sample.txt\" # Changed to BOW_test_sample.txt\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    excel_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip'\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- NEW: Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# Create a mapping for your specific input labels\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other' # Ensure 'other' also goes through for consistency\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels (should be fewer now)\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# Sample data to keep it less than or equal to 20 for quick testing\n",
    "# min(20, len(fr_data)) ensures we don't try to sample more rows than available\n",
    "fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded and sampled {len(fr_data)} functional reviews for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function (adapted for this cell) ---\n",
    "def classify_with_llama2(review_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama Llama 2 model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    # Format the prompt with the current review text\n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": LLAMA2_MODEL_NAME,\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False, # Get full response at once\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep temperature low for consistent classification\n",
    "            \"num_predict\": 100 # Limit output length to prevent rambling\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120) # Added timeout\n",
    "        response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: {review_text[:50]}...\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Evaluation Loop ---\n",
    "print(\"\\n========== Starting Classification Evaluation ==========\")\n",
    "\n",
    "predictions = []\n",
    "start_time = time.time()\n",
    "\n",
    "for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {LLAMA2_MODEL_NAME}\"):\n",
    "    response_data = classify_with_llama2(row['review'])\n",
    "    \n",
    "    if response_data[\"success\"]:\n",
    "        predicted_raw = response_data[\"raw_response\"].strip()\n",
    "        \n",
    "        # Regex to capture the exact category name after \"Classification:\"\n",
    "        match = re.search(\n",
    "            r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\", # Non-capturing group for \"CLASSIFICATION: \" is optional\n",
    "            predicted_raw,\n",
    "            re.IGNORECASE | re.DOTALL\n",
    "        )\n",
    "        \n",
    "        pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "        \n",
    "        # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "        # print(f\"Raw LLM Output for '{row['review'][:50]}...': '{predicted_raw}' -> Parsed: '{pred}'\")\n",
    "\n",
    "    else:\n",
    "        pred = \"Failed\"\n",
    "        logger.warning(f\"Classification failed for review: {row['review'][:50]}... Error: {response_data['raw_response']}\")\n",
    "    \n",
    "    predictions.append(pred)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n✅ Classification completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "# --- 7. Prepare Results and Generate Classification Report ---\n",
    "results_df = fr_data.copy()\n",
    "results_df['predicted'] = predictions\n",
    "\n",
    "# Filter out failed responses and predictions not in VALID_FR_LABELS for accurate report\n",
    "filtered_results = results_df[\n",
    "    (results_df['predicted'] != 'failed') &\n",
    "    (results_df['predicted'].isin(VALID_FR_LABELS))\n",
    "]\n",
    "\n",
    "print(\"\\n--- Sample of Predictions ---\")\n",
    "print(results_df.head())\n",
    "\n",
    "print(f\"\\n--- Classification Report for {LLAMA2_MODEL_NAME} ---\")\n",
    "if not filtered_results.empty:\n",
    "    report = classification_report(\n",
    "        filtered_results['ground_truth'],\n",
    "        filtered_results['predicted'],\n",
    "        labels=VALID_FR_LABELS, # Explicitly list labels to ensure all are shown, even if no predictions\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"No valid predictions to generate a classification report.\")\n",
    "    \n",
    "print(\"\\n--- Evaluation Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46c62d75-2b5f-42ea-b32f-5a0b318dc1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 20 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0  'the current fb app is not good at all for tab...   bug report\n",
      "1  'the problem is with the way items displaypics...   bug report\n",
      "2             'not to mention it force closes often'   bug report\n",
      "3  'also every time i open it asks me if i want t...   bug report\n",
      "4                     'now i cannot view any photos'   bug report\n",
      "----------------------------------------\n",
      "\n",
      "==================== Starting Classification Evaluation for Model: llama2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2: 100%|█████████████████████████████████████████████████| 20/20 [00:55<00:00,  2.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama2 completed in 0.93 minutes\n",
      "\n",
      "--- Sample of Predictions for llama2 ---\n",
      "                                              review ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...   bug report   \n",
      "1  'the problem is with the way items displaypics...   bug report   \n",
      "2             'not to mention it force closes often'   bug report   \n",
      "3  'also every time i open it asks me if i want t...   bug report   \n",
      "4                     'now i cannot view any photos'   bug report   \n",
      "\n",
      "         predicted  \n",
      "0       bug report  \n",
      "1       bug report  \n",
      "2       bug report  \n",
      "3  feature request  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for llama2 ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.60      0.43      0.50         7\n",
      "     bug report       0.33      0.83      0.48         6\n",
      "          other       0.00      0.00      0.00         7\n",
      "\n",
      "       accuracy                           0.40        20\n",
      "      macro avg       0.31      0.42      0.33        20\n",
      "   weighted avg       0.31      0.40      0.32        20\n",
      "\n",
      "\n",
      "==================== Evaluation for llama2 Complete ====================\n",
      "\n",
      "\n",
      "==================== Starting Classification Evaluation for Model: mistral ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with mistral: 100%|████████████████████████████████████████████████| 20/20 [00:52<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with mistral completed in 0.88 minutes\n",
      "\n",
      "--- Sample of Predictions for mistral ---\n",
      "                                              review ground_truth   predicted\n",
      "0  'the current fb app is not good at all for tab...   bug report       other\n",
      "1  'the problem is with the way items displaypics...   bug report       other\n",
      "2             'not to mention it force closes often'   bug report  bug report\n",
      "3  'also every time i open it asks me if i want t...   bug report       other\n",
      "4                     'now i cannot view any photos'   bug report  bug report\n",
      "\n",
      "--- Classification Report for mistral ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       1.00      0.71      0.83         7\n",
      "     bug report       0.60      0.50      0.55         6\n",
      "          other       0.60      0.86      0.71         7\n",
      "\n",
      "       accuracy                           0.70        20\n",
      "      macro avg       0.73      0.69      0.69        20\n",
      "   weighted avg       0.74      0.70      0.70        20\n",
      "\n",
      "\n",
      "==================== Evaluation for mistral Complete ====================\n",
      "\n",
      "\n",
      "==================== Starting Classification Evaluation for Model: llama3:8b ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama3:8b: 100%|██████████████████████████████████████████████| 20/20 [01:02<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with llama3:8b completed in 1.05 minutes\n",
      "\n",
      "--- Sample of Predictions for llama3:8b ---\n",
      "                                              review ground_truth   predicted\n",
      "0  'the current fb app is not good at all for tab...   bug report  bug report\n",
      "1  'the problem is with the way items displaypics...   bug report  bug report\n",
      "2             'not to mention it force closes often'   bug report  bug report\n",
      "3  'also every time i open it asks me if i want t...   bug report  bug report\n",
      "4                     'now i cannot view any photos'   bug report  bug report\n",
      "\n",
      "--- Classification Report for llama3:8b ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       1.00      0.14      0.25         7\n",
      "     bug report       0.32      1.00      0.48         6\n",
      "          other       0.00      0.00      0.00         7\n",
      "\n",
      "       accuracy                           0.35        20\n",
      "      macro avg       0.44      0.38      0.24        20\n",
      "   weighted avg       0.44      0.35      0.23        20\n",
      "\n",
      "\n",
      "==================== Evaluation for llama3:8b Complete ====================\n",
      "\n",
      "\n",
      "==================== Starting Classification Evaluation for Model: gemma:7b ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with gemma:7b: 100%|███████████████████████████████████████████████| 20/20 [01:17<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with gemma:7b completed in 1.29 minutes\n",
      "\n",
      "--- Sample of Predictions for gemma:7b ---\n",
      "                                              review ground_truth  \\\n",
      "0  'the current fb app is not good at all for tab...   bug report   \n",
      "1  'the problem is with the way items displaypics...   bug report   \n",
      "2             'not to mention it force closes often'   bug report   \n",
      "3  'also every time i open it asks me if i want t...   bug report   \n",
      "4                     'now i cannot view any photos'   bug report   \n",
      "\n",
      "         predicted  \n",
      "0  feature request  \n",
      "1  feature request  \n",
      "2       bug report  \n",
      "3  feature request  \n",
      "4       bug report  \n",
      "\n",
      "--- Classification Report for gemma:7b ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       0.42      0.71      0.53         7\n",
      "     bug report       0.38      0.50      0.43         6\n",
      "          other       0.00      0.00      0.00         7\n",
      "\n",
      "       accuracy                           0.40        20\n",
      "      macro avg       0.26      0.40      0.32        20\n",
      "   weighted avg       0.26      0.40      0.31        20\n",
      "\n",
      "\n",
      "==================== Evaluation for gemma:7b Complete ====================\n",
      "\n",
      "\n",
      "==================== Starting Classification Evaluation for Model: phi3:mini ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with phi3:mini: 100%|██████████████████████████████████████████████| 20/20 [00:49<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Classification with phi3:mini completed in 0.82 minutes\n",
      "\n",
      "--- Sample of Predictions for phi3:mini ---\n",
      "                                              review ground_truth   predicted\n",
      "0  'the current fb app is not good at all for tab...   bug report       other\n",
      "1  'the problem is with the way items displaypics...   bug report  bug report\n",
      "2             'not to mention it force closes often'   bug report  bug report\n",
      "3  'also every time i open it asks me if i want t...   bug report  bug report\n",
      "4                     'now i cannot view any photos'   bug report  bug report\n",
      "\n",
      "--- Classification Report for phi3:mini ---\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "feature request       1.00      0.43      0.60         7\n",
      "     bug report       0.33      0.83      0.48         6\n",
      "          other       0.50      0.14      0.22         7\n",
      "\n",
      "       accuracy                           0.45        20\n",
      "      macro avg       0.61      0.47      0.43        20\n",
      "   weighted avg       0.62      0.45      0.43        20\n",
      "\n",
      "\n",
      "==================== Evaluation for phi3:mini Complete ====================\n",
      "\n",
      "\n",
      "\n",
      "========== ALL MODELS EVALUATION COMPLETE ==========\n",
      "\n",
      "Summary of Accuracies:\n",
      "llama2: Accuracy = 0.40\n",
      "mistral: Accuracy = 0.70\n",
      "llama3:8b: Accuracy = 0.35\n",
      "gemma:7b: Accuracy = 0.40\n",
      "phi3:mini: Accuracy = 0.45\n",
      "\n",
      "--- Final Evaluation End ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os # Just in case it's needed for path resolution or similar\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test_sample.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# This mapping covers common variations found in your dataset\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# --- IMPORTANT: Removed the sampling line here to process the full dataset ---\n",
    "# fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Consistent Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name, # Use the model name passed as argument\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models ---\n",
    "all_models_results = {}\n",
    "\n",
    "for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*20} Starting Classification Evaluation for Model: {current_model_name} {'='*20}\")\n",
    "    \n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name}\"):\n",
    "        response_data = classify_with_ollama_model(row['review'], current_model_name)\n",
    "        \n",
    "        if response_data[\"success\"]:\n",
    "            predicted_raw = response_data[\"raw_response\"].strip()\n",
    "            \n",
    "            # Regex to capture the exact category name after \"Classification:\"\n",
    "            match = re.search(\n",
    "                r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\",\n",
    "                predicted_raw,\n",
    "                re.IGNORECASE | re.DOTALL\n",
    "            )\n",
    "            \n",
    "            pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "            \n",
    "            # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "            # if pred == \"Failed Parsing\":\n",
    "            #    print(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "\n",
    "        else:\n",
    "            pred = \"Failed\"\n",
    "            logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name}\")\n",
    "        \n",
    "        predictions.append(pred)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Classification with {current_model_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "    # --- 7. Prepare Results and Generate Classification Report for current model ---\n",
    "    results_df_current_model = fr_data.copy()\n",
    "    results_df_current_model['predicted'] = predictions\n",
    "\n",
    "    filtered_results = results_df_current_model[\n",
    "        (results_df_current_model['predicted'] != 'failed') &\n",
    "        (results_df_current_model['predicted'].isin(VALID_FR_LABELS))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n--- Sample of Predictions for {current_model_name} ---\")\n",
    "    print(results_df_current_model.head())\n",
    "\n",
    "    print(f\"\\n--- Classification Report for {current_model_name} ---\")\n",
    "    if not filtered_results.empty:\n",
    "        report = classification_report(\n",
    "            filtered_results['ground_truth'],\n",
    "            filtered_results['predicted'],\n",
    "            labels=VALID_FR_LABELS,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(report)\n",
    "        all_models_results[current_model_name] = {\n",
    "            'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "            'report': report # Store the full report string\n",
    "        }\n",
    "    else:\n",
    "        print(f\"No valid predictions to generate a classification report for {current_model_name}.\")\n",
    "        all_models_results[current_model_name] = {\n",
    "            'accuracy': 0.0,\n",
    "            'report': \"No valid predictions.\"\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n{'='*20} Evaluation for {current_model_name} Complete {'='*20}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL MODELS EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies:\")\n",
    "# Using accuracy_score here requires it to be imported. Add it to imports:\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score # Add this import\n",
    "for model, metrics in all_models_results.items():\n",
    "    print(f\"{model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "080415d6-4093-476e-ae3e-3c8a213c018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\n",
      "Loaded 512 functional reviews from the full dataset for testing.\n",
      "Sample of loaded FR data:\n",
      "                                              review ground_truth\n",
      "0                'this version crashes all the time'   bug report\n",
      "1                    'it take a lot time in loading'   bug report\n",
      "2                               'pages freeze often'   bug report\n",
      "3  'still having problems uploading sometimes tho...   bug report\n",
      "4  'it wont load any of my notifications when i c...   bug report\n",
      "----------------------------------------\n",
      "\n",
      "==================== Starting Classification Evaluation for Model: llama2 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying reviews with llama2:   1%|▎                                                | 3/512 [00:16<46:36,  5.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 135\u001b[0m\n\u001b[0;32m    132\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(fr_data\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(fr_data), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassifying reviews with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 135\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[43mclassify_with_ollama_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuccess\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    138\u001b[0m         predicted_raw \u001b[38;5;241m=\u001b[39m response_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_response\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstrip()\n",
      "Cell \u001b[1;32mIn[6], line 108\u001b[0m, in \u001b[0;36mclassify_with_ollama_model\u001b[1;34m(review_text, model_name)\u001b[0m\n\u001b[0;32m     97\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model_name, \u001b[38;5;66;03m# Use the model name passed as argument\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: formatted_prompt,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m     }\n\u001b[0;32m    105\u001b[0m }\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 108\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    110\u001b[0m     result \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:495\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[0;32m    508\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBrokenPipeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:398\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[1;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m header, value \u001b[38;5;129;01min\u001b[39;00m headers\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[1;32m--> 398\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1331\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[1;32m-> 1331\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1091\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[1;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[0;32m   1089\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer)\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[1;32m-> 1091\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1094\u001b[0m \n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(message_body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m   1097\u001b[0m         \u001b[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001b[39;00m\n\u001b[0;32m   1098\u001b[0m         \u001b[38;5;66;03m# is needed to allow the current position of mmap'ed\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;66;03m# files to be taken into account.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1035\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[1;32m-> 1035\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1037\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NotConnected()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[0;32m    238\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_connected_to_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:196\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \n\u001b[0;32m    193\u001b[0m \u001b[38;5;124;03m:return: New socket connection.\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\connection.py:81\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     79\u001b[0m         err \u001b[38;5;241m=\u001b[39m _\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m             \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\socket.py:501\u001b[0m, in \u001b[0;36msocket.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_real_close\u001b[39m(\u001b[38;5;28mself\u001b[39m, _ss\u001b[38;5;241m=\u001b[39m_socket\u001b[38;5;241m.\u001b[39msocket):\n\u001b[0;32m    498\u001b[0m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m     _ss\u001b[38;5;241m.\u001b[39mclose(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;66;03m# This function should not reference any globals. See issue #808164.\u001b[39;00m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_refs \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os # Just in case it's needed for path resolution or similar\n",
    "\n",
    "# --- 1. Logging Setup ---\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 2. LLM Configuration ---\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Define the list of Ollama models to test\n",
    "OLLAMA_MODELS_TO_TEST = [\n",
    "    \"llama2\",\n",
    "    \"mistral\",\n",
    "    \"llama3:8b\", # Assuming you pulled llama3:8b\n",
    "    \"gemma:7b\",\n",
    "    \"phi3:mini\"\n",
    "]\n",
    "\n",
    "# --- 3. Data Loading and Preparation (Full Dataset) ---\n",
    "print(\"--- Loading and preparing Functional Requirements data from BOW_test_sample.txt (Full Dataset) ---\")\n",
    "data_file_path = \"datasets/BOW_test.txt\"\n",
    "\n",
    "# Load the data, assuming 'review_text,classification' format\n",
    "fr_data_raw = pd.read_csv(\n",
    "    data_file_path,\n",
    "    sep=',',\n",
    "    header=None,\n",
    "    names=['review', 'ground_truth'],\n",
    "    on_bad_lines='skip' # Ignore malformed lines\n",
    ")\n",
    "\n",
    "# Define valid FR categories (lowercase for standardization)\n",
    "VALID_FR_LABELS = [\"feature request\", \"bug report\", \"other\"]\n",
    "\n",
    "# --- Standardize labels from the file to match VALID_FR_LABELS ---\n",
    "# This mapping covers common variations found in your dataset\n",
    "label_mapping = {\n",
    "    'bugreport': 'bug report',\n",
    "    'featurerequest': 'feature request',\n",
    "    'other': 'other'\n",
    "}\n",
    "fr_data_raw['ground_truth'] = fr_data_raw['ground_truth'].str.strip().str.lower().replace(label_mapping)\n",
    "# --- END NEW ---\n",
    "\n",
    "# Filter out rows where ground_truth is not one of our valid labels after mapping\n",
    "initial_len = len(fr_data_raw)\n",
    "fr_data = fr_data_raw[fr_data_raw['ground_truth'].isin(VALID_FR_LABELS)].reset_index(drop=True)\n",
    "\n",
    "if len(fr_data) < initial_len:\n",
    "    print(f\"Warning: Removed {initial_len - len(fr_data)} rows with unknown or invalid 'ground_truth' labels after mapping.\")\n",
    "\n",
    "# --- IMPORTANT: Removed the sampling line here to process the full dataset ---\n",
    "# fr_data = fr_data.sample(n=min(20, len(fr_data)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(f\"Loaded {len(fr_data)} functional reviews from the full dataset for testing.\")\n",
    "print(\"Sample of loaded FR data:\")\n",
    "print(fr_data.head())\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# --- 4. The Consistent Classification Prompt ---\n",
    "classification_prompt_text = \"\"\"\n",
    "You are an expert in software requirements analysis, specializing in user feedback. Your task is to precisely classify the provided app review segment into one of the following functional requirement categories: 'Feature Request', 'Bug Report', or 'Other'.\n",
    "\n",
    "**DEFINITIONS:**\n",
    "* **Feature Request**: This category is for user feedback that clearly suggests a **NEW** functionality, an **enhancement**, or an **improvement** to existing features that are **NOT currently broken or causing an error**. It describes something the user *wants the app to do* that it doesn't do yet, or a way to make an existing, working feature better.\n",
    "* **Bug Report**: This category is for user feedback that describes an **ERROR, FAULT, FLAW, or UNINTENDED BEHAVIOR** in the app. It highlights something that is **BROKEN**, not working as designed, or causing an incorrect/unexpected result.\n",
    "* **Other**: This category is for general feedback, compliments, complaints that are not specific enough to be a bug or feature, questions, or irrelevant comments.\n",
    "\n",
    "**INSTRUCTIONS:**\n",
    "1.  Read the \"App Review Segment\" carefully.\n",
    "2.  Determine which of the three categories (Feature Request, Bug Report, Other) it *most accurately* fits based on the provided definitions.\n",
    "3.  Your final output MUST be only the category name, without any additional text, explanation, or punctuation.\n",
    "\n",
    "**App Review Segment:** '''{review_text}'''\n",
    "\n",
    "**Classification:**\n",
    "\"\"\"\n",
    "\n",
    "# --- 5. LLM Interaction Function ---\n",
    "def classify_with_ollama_model(review_text: str, model_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sends a classification request to the local Ollama model.\n",
    "    \"\"\"\n",
    "    url = f\"{OLLAMA_BASE_URL}/api/generate\"\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    \n",
    "    formatted_prompt = classification_prompt_text.format(review_text=review_text)\n",
    "\n",
    "    data = {\n",
    "        \"model\": model_name, # Use the model name passed as argument\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.0, # Keep low for consistent classification\n",
    "            \"num_predict\": 100\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data), timeout=120)\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return {\"success\": True, \"raw_response\": result.get(\"response\", \"\")}\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        logger.error(f\"Failed to connect to Ollama server at {OLLAMA_BASE_URL}. Is Ollama running?\")\n",
    "        return {\"success\": False, \"raw_response\": \"Connection Error: Ollama server not reachable.\"}\n",
    "    except requests.exceptions.Timeout:\n",
    "        logger.error(f\"Ollama request timed out for review: '{review_text[:50]}...' with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": \"Timeout Error: Ollama request took too long.\"}\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logger.error(f\"HTTP error occurred: {http_err} - {response.text} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"HTTP Error: {http_err}\"}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during Ollama call: {e} with model {model_name}\")\n",
    "        return {\"success\": False, \"raw_response\": f\"Unexpected Error: {e}\"}\n",
    "\n",
    "# --- 6. Main Evaluation Loop for All Models ---\n",
    "all_models_results = {}\n",
    "\n",
    "for current_model_name in OLLAMA_MODELS_TO_TEST:\n",
    "    print(f\"\\n{'='*20} Starting Classification Evaluation for Model: {current_model_name} {'='*20}\")\n",
    "    \n",
    "    predictions = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, row in tqdm(fr_data.iterrows(), total=len(fr_data), desc=f\"Classifying reviews with {current_model_name}\"):\n",
    "        response_data = classify_with_ollama_model(row['review'], current_model_name)\n",
    "        \n",
    "        if response_data[\"success\"]:\n",
    "            predicted_raw = response_data[\"raw_response\"].strip()\n",
    "            \n",
    "            # Regex to capture the exact category name after \"Classification:\"\n",
    "            match = re.search(\n",
    "                r\"(?:CLASSIFICATION:\\s*)?(Feature Request|Bug Report|Other)\",\n",
    "                predicted_raw,\n",
    "                re.IGNORECASE | re.DOTALL\n",
    "            )\n",
    "            \n",
    "            pred = match.group(1).strip().lower() if match else \"Failed Parsing\"\n",
    "            \n",
    "            # Optional: Print raw output for debugging if needed (comment out for clean runs)\n",
    "            # if pred == \"Failed Parsing\":\n",
    "            #    print(f\"\\nFailed Parsing for: '{row['review']}'\\nRaw LLM Output: '{predicted_raw}'\")\n",
    "\n",
    "        else:\n",
    "            pred = \"Failed\"\n",
    "            logger.warning(f\"Classification failed for review: '{row['review'][:50]}...' with model {current_model_name}\")\n",
    "        \n",
    "        predictions.append(pred)\n",
    "\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n✅ Classification with {current_model_name} completed in {elapsed/60:.2f} minutes\")\n",
    "\n",
    "    # --- 7. Prepare Results and Generate Classification Report for current model ---\n",
    "    results_df_current_model = fr_data.copy()\n",
    "    results_df_current_model['predicted'] = predictions\n",
    "\n",
    "    filtered_results = results_df_current_model[\n",
    "        (results_df_current_model['predicted'] != 'failed') &\n",
    "        (results_df_current_model['predicted'].isin(VALID_FR_LABELS))\n",
    "    ]\n",
    "\n",
    "    print(f\"\\n--- Sample of Predictions for {current_model_name} ---\")\n",
    "    print(results_df_current_model.head())\n",
    "\n",
    "    print(f\"\\n--- Classification Report for {current_model_name} ---\")\n",
    "    if not filtered_results.empty:\n",
    "        report = classification_report(\n",
    "            filtered_results['ground_truth'],\n",
    "            filtered_results['predicted'],\n",
    "            labels=VALID_FR_LABELS,\n",
    "            zero_division=0\n",
    "        )\n",
    "        print(report)\n",
    "        all_models_results[current_model_name] = {\n",
    "            'accuracy': accuracy_score(filtered_results['ground_truth'], filtered_results['predicted']),\n",
    "            'report': report # Store the full report string\n",
    "        }\n",
    "    else:\n",
    "        print(f\"No valid predictions to generate a classification report for {current_model_name}.\")\n",
    "        all_models_results[current_model_name] = {\n",
    "            'accuracy': 0.0,\n",
    "            'report': \"No valid predictions.\"\n",
    "        }\n",
    "    \n",
    "    print(f\"\\n{'='*20} Evaluation for {current_model_name} Complete {'='*20}\\n\")\n",
    "\n",
    "print(\"\\n\\n========== ALL MODELS EVALUATION COMPLETE ==========\\n\")\n",
    "print(\"Summary of Accuracies:\")\n",
    "# Using accuracy_score here requires it to be imported. Add it to imports:\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.metrics import accuracy_score # Add this import\n",
    "for model, metrics in all_models_results.items():\n",
    "    print(f\"{model}: Accuracy = {metrics['accuracy']:.2f}\")\n",
    "\n",
    "print(\"\\n--- Final Evaluation End ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea011add-22ee-41be-8b08-8d488af9a612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
